================================================================================
ENTROPY ANALYSIS: Testing Syntactic Constraint
================================================================================

================================================================================
ANALYSIS 1: ENTROPY COMPARISON
================================================================================

Question: Does syntax constrain predictions (reduce entropy)?
Test: Δ(Jabberwocky - Scrambled Jabberwocky)

Jabberwocky (Syntax):          8.260 ± 0.150 bits (entropy)
Scrambled (No Structure):      8.376 ± 0.177 bits (entropy)
Δ Entropy (Syntax - Scrambled): -0.116 bits

Interpretation: Syntax REDUCES entropy (increases constraint)

For comparison:
Jabberwocky surprisal:         14.024 ± 0.288 bits
Scrambled surprisal:           14.243 ± 0.300 bits
Δ Surprisal:                   -0.219 bits

Effect sizes (Cohen's d):
  Entropy:   d = -0.129
  Surprisal: d = -0.136

================================================================================
ANALYSIS 2: FUNCTION-WORD vs CONTENT-WORD SPLIT
================================================================================

Question: Does structure matter more for function-word targets?
Prediction: Structure should constrain function words more than content words

Function-word positions:
  Entropy:   Jab 9.781 vs Scram 8.378 → Δ = 1.403 bits
  Surprisal: Jab 13.969 vs Scram 13.822 → Δ = 0.147 bits

Content-word positions:
  Entropy:   Jab 6.831 vs Scram 8.385 → Δ = -1.554 bits
  Surprisal: Jab 13.881 vs Scram 14.512 → Δ = -0.631 bits

================================================================================
ANALYSIS 3: CONFIDENT-WRONG DIAGNOSTIC
================================================================================

Question: Does structure make models confident but wrong?
Signature: Lower entropy (more committed) but similar/higher surprisal (still wrong)

Condition                      |  Entropy |  Surprisal |  Gap (S-E)
----------------------------------------------------------------------
sentence                       |    7.405 |      6.884 |     -0.522
jabberwocky_matched            |    8.260 |     14.024 |      5.764
scrambled_jabberwocky          |    8.376 |     14.243 |      5.867
word_list_nonce_2tok           |    8.841 |     12.254 |      3.413

Interpretation:
  Entropy: Model's uncertainty (bits). Lower = more committed.
  Surprisal: Model's error (bits). Higher = worse prediction.
  Gap (S-E): 'Confident-wrong' index. Positive = confident but wrong.

If Jabberwocky has:
  - Lower entropy than Scrambled → structure increases commitment
  - Similar/higher surprisal → but doesn't help with nonce identity
  → Structure engages prediction without improving lexical accuracy

================================================================================
SUMMARY
================================================================================

Key Result 1: Δ Entropy (Jabberwocky - Scrambled) = -0.116 bits
  → Syntax REDUCES entropy (increases constraint) ✓

Key Result 2: Δ Surprisal (Jabberwocky - Scrambled) = -0.219 bits
  → Syntax does NOT help predict nonce identity

Conclusion:
  Syntax constrains predictions (↓ entropy) without improving
  nonce accuracy (~ surprisal). This is the 'confident-wrong'
  signature: structure engages prediction but can't overcome
  the fundamental unpredictability of novel lexical items.

================================================================================
