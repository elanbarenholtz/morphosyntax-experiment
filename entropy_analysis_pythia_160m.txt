================================================================================
ENTROPY ANALYSIS: Testing Syntactic Constraint
================================================================================

================================================================================
ANALYSIS 1: ENTROPY COMPARISON
================================================================================

Question: Does syntax constrain predictions (reduce entropy)?
Test: Δ(Jabberwocky - Scrambled Jabberwocky)

Jabberwocky (Syntax):          8.523 ± 0.138 bits (entropy)
Scrambled (No Structure):      8.775 ± 0.142 bits (entropy)
Δ Entropy (Syntax - Scrambled): -0.252 bits

Interpretation: Syntax REDUCES entropy (increases constraint)

For comparison:
Jabberwocky surprisal:         13.191 ± 0.275 bits
Scrambled surprisal:           13.422 ± 0.258 bits
Δ Surprisal:                   -0.231 bits

Effect sizes (Cohen's d):
  Entropy:   d = -0.329
  Surprisal: d = -0.159

================================================================================
ANALYSIS 2: FUNCTION-WORD vs CONTENT-WORD SPLIT
================================================================================

Question: Does structure matter more for function-word targets?
Prediction: Structure should constrain function words more than content words

Function-word positions:
  Entropy:   Jab 10.057 vs Scram 8.917 → Δ = 1.140 bits
  Surprisal: Jab 13.095 vs Scram 13.605 → Δ = -0.510 bits

Content-word positions:
  Entropy:   Jab 7.159 vs Scram 8.680 → Δ = -1.521 bits
  Surprisal: Jab 13.247 vs Scram 13.212 → Δ = 0.035 bits

================================================================================
ANALYSIS 3: CONFIDENT-WRONG DIAGNOSTIC
================================================================================

Question: Does structure make models confident but wrong?
Signature: Lower entropy (more committed) but similar/higher surprisal (still wrong)

Condition                      |  Entropy |  Surprisal |  Gap (S-E)
----------------------------------------------------------------------
sentence                       |    7.730 |      7.397 |     -0.333
jabberwocky_matched            |    8.523 |     13.191 |      4.668
scrambled_jabberwocky          |    8.775 |     13.422 |      4.648
word_list_nonce_2tok           |    9.609 |     12.531 |      2.922

Interpretation:
  Entropy: Model's uncertainty (bits). Lower = more committed.
  Surprisal: Model's error (bits). Higher = worse prediction.
  Gap (S-E): 'Confident-wrong' index. Positive = confident but wrong.

If Jabberwocky has:
  - Lower entropy than Scrambled → structure increases commitment
  - Similar/higher surprisal → but doesn't help with nonce identity
  → Structure engages prediction without improving lexical accuracy

================================================================================
SUMMARY
================================================================================

Key Result 1: Δ Entropy (Jabberwocky - Scrambled) = -0.252 bits
  → Syntax REDUCES entropy (increases constraint) ✓

Key Result 2: Δ Surprisal (Jabberwocky - Scrambled) = -0.231 bits
  → Syntax does NOT help predict nonce identity

Conclusion:
  Syntax constrains predictions (↓ entropy) without improving
  nonce accuracy (~ surprisal). This is the 'confident-wrong'
  signature: structure engages prediction but can't overcome
  the fundamental unpredictability of novel lexical items.

================================================================================
