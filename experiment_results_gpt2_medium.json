[
  {
    "set_id": 1,
    "conditions": {
      "sentence": {
        "text": "The teacher was explaining the concept to the students clearly",
        "tokens": [
          "The",
          " teacher",
          " was",
          " explaining",
          " the",
          " concept",
          " to",
          " the",
          " students",
          " clearly"
        ],
        "num_tokens": 10,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The nellick was trening the trell to the geffs blundlely",
        "tokens": [
          "The",
          " ne",
          "ll",
          "ick",
          " was",
          " tre",
          "ning",
          " the",
          " tre",
          "ll",
          " to",
          " the",
          " ge",
          "ff",
          "s",
          " bl",
          "undle",
          "ly"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "freff nellick keff tren freff trell grofl freff geff blundle",
        "tokens": [
          "fre",
          "ff",
          " ne",
          "ll",
          "ick",
          " ke",
          "ff",
          " t",
          "ren",
          " fre",
          "ff",
          " tre",
          "ll",
          " gro",
          "fl",
          " fre",
          "ff",
          " ge",
          "ff",
          " bl",
          "undle"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "zemble zrell kren bliv wreffin cremble plenth drell quorb heffin",
        "tokens": [
          "z",
          "em",
          "ble",
          " z",
          "rell",
          " k",
          "ren",
          " bl",
          "iv",
          " wre",
          "ff",
          "in",
          " cre",
          "mble",
          " pl",
          "enth",
          " d",
          "rell",
          " qu",
          "orb",
          " he",
          "ff",
          "in"
        ],
        "num_tokens": 23,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 2,
    "conditions": {
      "sentence": {
        "text": "A small bird landed softly on the wooden fence",
        "tokens": [
          "A",
          " small",
          " bird",
          " landed",
          " softly",
          " on",
          " the",
          " wooden",
          " fence"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A yendle vork whiffed glorply on the driv moxen",
        "tokens": [
          "A",
          " y",
          "end",
          "le",
          " v",
          "ork",
          " whiff",
          "ed",
          " glor",
          "ply",
          " on",
          " the",
          " dri",
          "v",
          " m",
          "ox",
          "en"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "vren yendle vork whiff glorp yalm hreff driv moxen",
        "tokens": [
          "v",
          "ren",
          " y",
          "end",
          "le",
          " v",
          "ork",
          " whiff",
          " gl",
          "orp",
          " y",
          "alm",
          " h",
          "re",
          "ff",
          " dri",
          "v",
          " m",
          "ox",
          "en"
        ],
        "num_tokens": 20,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "mornix cromple blenk drell plenth yeff remble trell gribble",
        "tokens": [
          "m",
          "orn",
          "ix",
          " c",
          "rom",
          "ple",
          " bl",
          "en",
          "k",
          " d",
          "rell",
          " pl",
          "enth",
          " ye",
          "ff",
          " rem",
          "ble",
          " tre",
          "ll",
          " g",
          "rib",
          "ble"
        ],
        "num_tokens": 22,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 3,
    "conditions": {
      "sentence": {
        "text": "The children were playing with their toys in the garden",
        "tokens": [
          "The",
          " children",
          " were",
          " playing",
          " with",
          " their",
          " toys",
          " in",
          " the",
          " garden"
        ],
        "num_tokens": 10,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The moxen were mornixing with their slenks in the trock",
        "tokens": [
          "The",
          " m",
          "ox",
          "en",
          " were",
          " m",
          "orn",
          "ix",
          "ing",
          " with",
          " their",
          " sl",
          "en",
          "ks",
          " in",
          " the",
          " tro",
          "ck"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "tembly moxen blenk mornix sniv blemble slenk fleff tembly trock",
        "tokens": [
          "tem",
          "bly",
          " m",
          "ox",
          "en",
          " bl",
          "en",
          "k",
          " m",
          "orn",
          "ix",
          " sn",
          "iv",
          " ble",
          "mble",
          " sl",
          "en",
          "k",
          " fle",
          "ff",
          " tem",
          "bly",
          " tro",
          "ck"
        ],
        "num_tokens": 24,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "heffin flenk demble grenth wreffin yendle frell glendle whill jendle",
        "tokens": [
          "he",
          "ff",
          "in",
          " fl",
          "en",
          "k",
          " dem",
          "ble",
          " gren",
          "th",
          " wre",
          "ff",
          "in",
          " y",
          "end",
          "le",
          " fre",
          "ll",
          " gl",
          "end",
          "le",
          " wh",
          "ill",
          " j",
          "end",
          "le"
        ],
        "num_tokens": 26,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 4,
    "conditions": {
      "sentence": {
        "text": "She has been reading that book very carefully",
        "tokens": [
          "She",
          " has",
          " been",
          " reading",
          " that",
          " book",
          " very",
          " carefully"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "krendle has been yalming that klim very zinkly",
        "tokens": [
          "k",
          "rend",
          "le",
          " has",
          " been",
          " y",
          "al",
          "ming",
          " that",
          " k",
          "lim",
          " very",
          " z",
          "ink",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "krendle drell cleff yalm yorbel klim grell zink",
        "tokens": [
          "k",
          "rend",
          "le",
          " d",
          "rell",
          " cle",
          "ff",
          " y",
          "alm",
          " y",
          "or",
          "bel",
          " k",
          "lim",
          " gre",
          "ll",
          " z",
          "ink"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "quorb tren plov nellick fleff wreffin jell snell",
        "tokens": [
          "qu",
          "orb",
          " t",
          "ren",
          " pl",
          "ov",
          " ne",
          "ll",
          "ick",
          " fle",
          "ff",
          " wre",
          "ff",
          "in",
          " j",
          "ell",
          " sn",
          "ell"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 5,
    "conditions": {
      "sentence": {
        "text": "The scientist discovered a new species in the forest",
        "tokens": [
          "The",
          " scientist",
          " discovered",
          " a",
          " new",
          " species",
          " in",
          " the",
          " forest"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The sprock ploved a moxen whiffs in the reffin",
        "tokens": [
          "The",
          " sp",
          "rock",
          " pl",
          "oved",
          " a",
          " m",
          "ox",
          "en",
          " wh",
          "iffs",
          " in",
          " the",
          " re",
          "ff",
          "in"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "veff sprock plov tembly moxen whiff pliff veff reffin",
        "tokens": [
          "ve",
          "ff",
          " sp",
          "rock",
          " pl",
          "ov",
          " tem",
          "bly",
          " m",
          "ox",
          "en",
          " whiff",
          " pl",
          "iff",
          " ve",
          "ff",
          " re",
          "ff",
          "in"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "freff kren teff grent wreffin sniv jembly grell remble",
        "tokens": [
          "fre",
          "ff",
          " k",
          "ren",
          " te",
          "ff",
          " g",
          "rent",
          " wre",
          "ff",
          "in",
          " sn",
          "iv",
          " j",
          "emb",
          "ly",
          " gre",
          "ll",
          " rem",
          "ble"
        ],
        "num_tokens": 20,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 6,
    "conditions": {
      "sentence": {
        "text": "They will finish the project by tomorrow morning",
        "tokens": [
          "They",
          " will",
          " finish",
          " the",
          " project",
          " by",
          " tomorrow",
          " morning"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "drell will deffin the yrell by crell mornixing",
        "tokens": [
          "d",
          "rell",
          " will",
          " de",
          "ff",
          "in",
          " the",
          " y",
          "rell",
          " by",
          " cre",
          "ll",
          " m",
          "orn",
          "ix",
          "ing"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "drell vemble deffin whisp yrell sniv crell mornix",
        "tokens": [
          "d",
          "rell",
          " ve",
          "mble",
          " de",
          "ff",
          "in",
          " wh",
          "isp",
          " y",
          "rell",
          " sn",
          "iv",
          " cre",
          "ll",
          " m",
          "orn",
          "ix"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "klorn gremble remble klorp gleb zell prell shill",
        "tokens": [
          "kl",
          "orn",
          " g",
          "rem",
          "ble",
          " rem",
          "ble",
          " k",
          "lor",
          "p",
          " gle",
          "b",
          " z",
          "ell",
          " pre",
          "ll",
          " sh",
          "ill"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 7,
    "conditions": {
      "sentence": {
        "text": "A gentle breeze was blowing through the open window",
        "tokens": [
          "A",
          " gentle",
          " breeze",
          " was",
          " blowing",
          " through",
          " the",
          " open",
          " window"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A demble wexin was zomping through the creff daffin",
        "tokens": [
          "A",
          " dem",
          "ble",
          " we",
          "x",
          "in",
          " was",
          " z",
          "omp",
          "ing",
          " through",
          " the",
          " cre",
          "ff",
          " d",
          "aff",
          "in"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "trock demble wexin blenth zomp brell jexen creff daffin",
        "tokens": [
          "t",
          "rock",
          " dem",
          "ble",
          " we",
          "x",
          "in",
          " bl",
          "enth",
          " z",
          "omp",
          " bre",
          "ll",
          " j",
          "ex",
          "en",
          " cre",
          "ff",
          " d",
          "aff",
          "in"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "remble bren flenk kreb lemble yorbel flib quemble klorp",
        "tokens": [
          "rem",
          "ble",
          " b",
          "ren",
          " fl",
          "en",
          "k",
          " k",
          "reb",
          " le",
          "mble",
          " y",
          "or",
          "bel",
          " fl",
          "ib",
          " qu",
          "em",
          "ble",
          " k",
          "lor",
          "p"
        ],
        "num_tokens": 22,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 8,
    "conditions": {
      "sentence": {
        "text": "The musician played the melody beautifully on stage",
        "tokens": [
          "The",
          " musician",
          " played",
          " the",
          " melody",
          " beautifully",
          " on",
          " stage"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The grenth wrembleed the freff plenthly on trell",
        "tokens": [
          "The",
          " gren",
          "th",
          " w",
          "rem",
          "ble",
          "ed",
          " the",
          " fre",
          "ff",
          " pl",
          "enth",
          "ly",
          " on",
          " tre",
          "ll"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "vemble grenth wremble vemble freff plenth yrell trell",
        "tokens": [
          "ve",
          "mble",
          " gren",
          "th",
          " w",
          "rem",
          "ble",
          " ve",
          "mble",
          " fre",
          "ff",
          " pl",
          "enth",
          " y",
          "rell",
          " tre",
          "ll"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "quorb flib flurn glendle frell horp zell drell",
        "tokens": [
          "qu",
          "orb",
          " fl",
          "ib",
          " fl",
          "urn",
          " gl",
          "end",
          "le",
          " fre",
          "ll",
          " hor",
          "p",
          " z",
          "ell",
          " d",
          "rell"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 9,
    "conditions": {
      "sentence": {
        "text": "He could have solved the problem more easily",
        "tokens": [
          "He",
          " could",
          " have",
          " solved",
          " the",
          " problem",
          " more",
          " easily"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "zell could have veffed the flib bren plonkly",
        "tokens": [
          "z",
          "ell",
          " could",
          " have",
          " ve",
          "ff",
          "ed",
          " the",
          " fl",
          "ib",
          " b",
          "ren",
          " pl",
          "on",
          "k",
          "ly"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "zell blenth lemble veff blenk flib bren plonk",
        "tokens": [
          "z",
          "ell",
          " bl",
          "enth",
          " le",
          "mble",
          " ve",
          "ff",
          " bl",
          "en",
          "k",
          " fl",
          "ib",
          " b",
          "ren",
          " pl",
          "on",
          "k"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "vork hemble trock reffin prell trell vreff horp",
        "tokens": [
          "v",
          "ork",
          " he",
          "mble",
          " tro",
          "ck",
          " re",
          "ff",
          "in",
          " pre",
          "ll",
          " tre",
          "ll",
          " v",
          "re",
          "ff",
          " hor",
          "p"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 10,
    "conditions": {
      "sentence": {
        "text": "The committee is reviewing the proposal thoroughly",
        "tokens": [
          "The",
          " committee",
          " is",
          " reviewing",
          " the",
          " proposal",
          " thoroughly"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The whisp is quendleing the rellick grellly",
        "tokens": [
          "The",
          " wh",
          "isp",
          " is",
          " qu",
          "end",
          "le",
          "ing",
          " the",
          " re",
          "ll",
          "ick",
          " gre",
          "ll",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "prell whisp urnble quendle prell rellick grell",
        "tokens": [
          "pre",
          "ll",
          " wh",
          "isp",
          " ",
          "urn",
          "ble",
          " qu",
          "end",
          "le",
          " pre",
          "ll",
          " re",
          "ll",
          "ick",
          " gre",
          "ll"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "lurnip wremble cremble tren slenk plenth zomp",
        "tokens": [
          "l",
          "urn",
          "ip",
          " w",
          "rem",
          "ble",
          " cre",
          "mble",
          " t",
          "ren",
          " sl",
          "en",
          "k",
          " pl",
          "enth",
          " z",
          "omp"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 11,
    "conditions": {
      "sentence": {
        "text": "A mysterious figure appeared suddenly in the darkness",
        "tokens": [
          "A",
          " mysterious",
          " figure",
          " appeared",
          " suddenly",
          " in",
          " the",
          " darkness"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A lurnips whill blorked klimly in the temblys",
        "tokens": [
          "A",
          " l",
          "urn",
          "ips",
          " wh",
          "ill",
          " bl",
          "ork",
          "ed",
          " k",
          "lim",
          "ly",
          " in",
          " the",
          " tem",
          "b",
          "lys"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "snorf lurnip whill blork klim prock sken tembly",
        "tokens": [
          "s",
          "nor",
          "f",
          " l",
          "urn",
          "ip",
          " wh",
          "ill",
          " bl",
          "ork",
          " k",
          "lim",
          " pro",
          "ck",
          " sk",
          "en",
          " tem",
          "bly"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "shill vren driv plib gribble plov urnix borp",
        "tokens": [
          "sh",
          "ill",
          " v",
          "ren",
          " dri",
          "v",
          " pl",
          "ib",
          " g",
          "rib",
          "ble",
          " pl",
          "ov",
          " ",
          "urn",
          "ix",
          " b",
          "orp"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 12,
    "conditions": {
      "sentence": {
        "text": "The students should study the material very seriously",
        "tokens": [
          "The",
          " students",
          " should",
          " study",
          " the",
          " material",
          " very",
          " seriously"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The blivs should sniv the mornix very whillly",
        "tokens": [
          "The",
          " bl",
          "iv",
          "s",
          " should",
          " sn",
          "iv",
          " the",
          " m",
          "orn",
          "ix",
          " very",
          " wh",
          "ill",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "whisp bliv vellop sniv whisp mornix gleb whill",
        "tokens": [
          "wh",
          "isp",
          " bl",
          "iv",
          " ve",
          "ll",
          "op",
          " sn",
          "iv",
          " wh",
          "isp",
          " m",
          "orn",
          "ix",
          " gle",
          "b",
          " wh",
          "ill"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "teff clorp tren yalp florp fleff skeff leppik",
        "tokens": [
          "te",
          "ff",
          " cl",
          "orp",
          " t",
          "ren",
          " y",
          "al",
          "p",
          " fl",
          "orp",
          " fle",
          "ff",
          " ske",
          "ff",
          " le",
          "pp",
          "ik"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 13,
    "conditions": {
      "sentence": {
        "text": "She was walking slowly through the quiet streets",
        "tokens": [
          "She",
          " was",
          " walking",
          " slowly",
          " through",
          " the",
          " quiet",
          " streets"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "jrenth was trening frellly through the grent keffs",
        "tokens": [
          "j",
          "ren",
          "th",
          " was",
          " tre",
          "ning",
          " fre",
          "ll",
          "ly",
          " through",
          " the",
          " g",
          "rent",
          " ke",
          "ff",
          "s"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "jrenth klim tren frell klorp gribble grent keff",
        "tokens": [
          "j",
          "ren",
          "th",
          " k",
          "lim",
          " t",
          "ren",
          " fre",
          "ll",
          " k",
          "lor",
          "p",
          " g",
          "rib",
          "ble",
          " g",
          "rent",
          " ke",
          "ff"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "fremble vren yeff whell creff veff grenth dwel",
        "tokens": [
          "f",
          "rem",
          "ble",
          " v",
          "ren",
          " ye",
          "ff",
          " whe",
          "ll",
          " cre",
          "ff",
          " ve",
          "ff",
          " gren",
          "th",
          " dw",
          "el"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 14,
    "conditions": {
      "sentence": {
        "text": "The engineers are designing a better system carefully",
        "tokens": [
          "The",
          " engineers",
          " are",
          " designing",
          " a",
          " better",
          " system",
          " carefully"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The zembles are skening a yendle dwel trockly",
        "tokens": [
          "The",
          " z",
          "em",
          "bles",
          " are",
          " sk",
          "ening",
          " a",
          " y",
          "end",
          "le",
          " dw",
          "el",
          " tro",
          "ck",
          "ly"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "creff zemble tren sken zeffin yendle dwel trock",
        "tokens": [
          "cre",
          "ff",
          " z",
          "em",
          "ble",
          " t",
          "ren",
          " sk",
          "en",
          " z",
          "eff",
          "in",
          " y",
          "end",
          "le",
          " dw",
          "el",
          " tro",
          "ck"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "grof blimp vork brell frell jexen whill trell",
        "tokens": [
          "gro",
          "f",
          " bl",
          "imp",
          " v",
          "ork",
          " bre",
          "ll",
          " fre",
          "ll",
          " j",
          "ex",
          "en",
          " wh",
          "ill",
          " tre",
          "ll"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 15,
    "conditions": {
      "sentence": {
        "text": "A young artist painted the landscape very skillfully",
        "tokens": [
          "A",
          " young",
          " artist",
          " painted",
          " the",
          " landscape",
          " very",
          " skill",
          "fully"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A drell jendle wreffined the pliff very slenkly",
        "tokens": [
          "A",
          " d",
          "rell",
          " j",
          "end",
          "le",
          " wre",
          "ff",
          "ined",
          " the",
          " pl",
          "iff",
          " very",
          " sl",
          "en",
          "k",
          "ly"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "grell drell jendle wreffin veff pliff prav slenk",
        "tokens": [
          "g",
          "rell",
          " d",
          "rell",
          " j",
          "end",
          "le",
          " wre",
          "ff",
          "in",
          " ve",
          "ff",
          " pl",
          "iff",
          " p",
          "rav",
          " sl",
          "en",
          "k"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "snell jrenth jell jembly demble fremble brell yorbel",
        "tokens": [
          "s",
          "nell",
          " j",
          "ren",
          "th",
          " j",
          "ell",
          " j",
          "emb",
          "ly",
          " dem",
          "ble",
          " fre",
          "mble",
          " bre",
          "ll",
          " y",
          "or",
          "bel"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 16,
    "conditions": {
      "sentence": {
        "text": "The dog had been barking loudly at the stranger",
        "tokens": [
          "The",
          " dog",
          " had",
          " been",
          " barking",
          " loudly",
          " at",
          " the",
          " stranger"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The fleff zeffin been yalming reffinly at the prock",
        "tokens": [
          "The",
          " fle",
          "ff",
          " z",
          "eff",
          "in",
          " been",
          " y",
          "al",
          "ming",
          " re",
          "ff",
          "in",
          "ly",
          " at",
          " the",
          " pro",
          "ck"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "vork fleff zeffin jendle yalm reffin snorf vork prock",
        "tokens": [
          "v",
          "ork",
          " fle",
          "ff",
          " z",
          "eff",
          "in",
          " j",
          "end",
          "le",
          " y",
          "alm",
          " re",
          "ff",
          "in",
          " sn",
          "orf",
          " v",
          "ork",
          " pro",
          "ck"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "grell blicket fremble yeff jrenth mornix hreff wuggle mip",
        "tokens": [
          "g",
          "rell",
          " bl",
          "icket",
          " fre",
          "mble",
          " ye",
          "ff",
          " j",
          "ren",
          "th",
          " m",
          "orn",
          "ix",
          " h",
          "re",
          "ff",
          " w",
          "uggle",
          " m",
          "ip"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 17,
    "conditions": {
      "sentence": {
        "text": "They were discussing the matter quite intensely",
        "tokens": [
          "They",
          " were",
          " discussing",
          " the",
          " matter",
          " quite",
          " intensely"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "pliff were praving the gremble horp wexinly",
        "tokens": [
          "pl",
          "iff",
          " were",
          " pra",
          "ving",
          " the",
          " g",
          "rem",
          "ble",
          " hor",
          "p",
          " we",
          "x",
          "in",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "pliff sniv prav flimp gremble horp wexin",
        "tokens": [
          "pl",
          "iff",
          " sn",
          "iv",
          " p",
          "rav",
          " fl",
          "imp",
          " g",
          "rem",
          "ble",
          " hor",
          "p",
          " we",
          "x",
          "in"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "crendle yalm crell screll urnble shill zemble",
        "tokens": [
          "cre",
          "nd",
          "le",
          " y",
          "alm",
          " cre",
          "ll",
          " sc",
          "rell",
          " ",
          "urn",
          "ble",
          " sh",
          "ill",
          " z",
          "em",
          "ble"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 18,
    "conditions": {
      "sentence": {
        "text": "The flowers are blooming beautifully in the spring",
        "tokens": [
          "The",
          " flowers",
          " are",
          " blo",
          "oming",
          " beautifully",
          " in",
          " the",
          " spring"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The wuggles are mornixing daxenly in the jelling",
        "tokens": [
          "The",
          " w",
          "ugg",
          "les",
          " are",
          " m",
          "orn",
          "ix",
          "ing",
          " d",
          "ax",
          "en",
          "ly",
          " in",
          " the",
          " j",
          "elling"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "prock wuggle neffin mornix daxen brell prock jell",
        "tokens": [
          "pro",
          "ck",
          " w",
          "uggle",
          " ne",
          "ff",
          "in",
          " m",
          "orn",
          "ix",
          " d",
          "ax",
          "en",
          " bre",
          "ll",
          " pro",
          "ck",
          " j",
          "ell"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "cleff whill remble grofl tamp verm heffin gribble",
        "tokens": [
          "cle",
          "ff",
          " wh",
          "ill",
          " rem",
          "ble",
          " gro",
          "fl",
          " tamp",
          " ver",
          "m",
          " he",
          "ff",
          "in",
          " g",
          "rib",
          "ble"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 19,
    "conditions": {
      "sentence": {
        "text": "A talented chef prepared the meal expertly",
        "tokens": [
          "A",
          " talented",
          " chef",
          " prepared",
          " the",
          " meal",
          " expert",
          "ly"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A blorked cremble krened the prock cromplely",
        "tokens": [
          "A",
          " bl",
          "ork",
          "ed",
          " cre",
          "mble",
          " k",
          "ren",
          "ed",
          " the",
          " pro",
          "ck",
          " c",
          "rom",
          "ple",
          "ly"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "leppik blork cremble kren screll prock cromple",
        "tokens": [
          "le",
          "pp",
          "ik",
          " bl",
          "ork",
          " cre",
          "mble",
          " k",
          "ren",
          " sc",
          "rell",
          " pro",
          "ck",
          " c",
          "rom",
          "ple"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "tren plenth trell pordel dwel flimp wexin",
        "tokens": [
          "t",
          "ren",
          " pl",
          "enth",
          " tre",
          "ll",
          " p",
          "ord",
          "el",
          " dw",
          "el",
          " fl",
          "imp",
          " we",
          "x",
          "in"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 20,
    "conditions": {
      "sentence": {
        "text": "The researchers have discovered the answer recently",
        "tokens": [
          "The",
          " researchers",
          " have",
          " discovered",
          " the",
          " answer",
          " recently"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The yendles have crelled the brell borply",
        "tokens": [
          "The",
          " y",
          "end",
          "les",
          " have",
          " cre",
          "lled",
          " the",
          " bre",
          "ll",
          " b",
          "or",
          "ply"
        ],
        "num_tokens": 13,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "plenth yendle vork crell plenth brell borp",
        "tokens": [
          "pl",
          "enth",
          " y",
          "end",
          "le",
          " v",
          "ork",
          " cre",
          "ll",
          " pl",
          "enth",
          " bre",
          "ll",
          " b",
          "orp"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "mornix sprock sken quendle whisp driv drell",
        "tokens": [
          "m",
          "orn",
          "ix",
          " sp",
          "rock",
          " sk",
          "en",
          " qu",
          "end",
          "le",
          " wh",
          "isp",
          " dri",
          "v",
          " d",
          "rell"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 21,
    "conditions": {
      "sentence": {
        "text": "She will be traveling to the conference tomorrow",
        "tokens": [
          "She",
          " will",
          " be",
          " traveling",
          " to",
          " the",
          " conference",
          " tomorrow"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "mip will keff quendleing to the wremble zeffin",
        "tokens": [
          "m",
          "ip",
          " will",
          " ke",
          "ff",
          " qu",
          "end",
          "le",
          "ing",
          " to",
          " the",
          " w",
          "rem",
          "ble",
          " z",
          "eff",
          "in"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "mip heffin keff quendle leppik skeff wremble zeffin",
        "tokens": [
          "m",
          "ip",
          " he",
          "ff",
          "in",
          " ke",
          "ff",
          " qu",
          "end",
          "le",
          " le",
          "pp",
          "ik",
          " ske",
          "ff",
          " w",
          "rem",
          "ble",
          " z",
          "eff",
          "in"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "reffin glorp zell jell vren trell trock urnble",
        "tokens": [
          "re",
          "ff",
          "in",
          " gl",
          "orp",
          " z",
          "ell",
          " j",
          "ell",
          " v",
          "ren",
          " tre",
          "ll",
          " tro",
          "ck",
          " ",
          "urn",
          "ble"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 22,
    "conditions": {
      "sentence": {
        "text": "The workers were building the structure very carefully",
        "tokens": [
          "The",
          " workers",
          " were",
          " building",
          " the",
          " structure",
          " very",
          " carefully"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The frells were quorbing the jell very lemblely",
        "tokens": [
          "The",
          " fre",
          "ll",
          "s",
          " were",
          " qu",
          "or",
          "bing",
          " the",
          " j",
          "ell",
          " very",
          " le",
          "mble",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "shill frell urnix quorb shill jell jrenth lemble",
        "tokens": [
          "sh",
          "ill",
          " fre",
          "ll",
          " ",
          "urn",
          "ix",
          " qu",
          "orb",
          " sh",
          "ill",
          " j",
          "ell",
          " j",
          "ren",
          "th",
          " le",
          "mble"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "sken quemble yalm blemble neffin screll blenth verm",
        "tokens": [
          "s",
          "ken",
          " qu",
          "em",
          "ble",
          " y",
          "alm",
          " ble",
          "mble",
          " ne",
          "ff",
          "in",
          " sc",
          "rell",
          " bl",
          "enth",
          " ver",
          "m"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 23,
    "conditions": {
      "sentence": {
        "text": "A bright light was shining through the window",
        "tokens": [
          "A",
          " bright",
          " light",
          " was",
          " shining",
          " through",
          " the",
          " window"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A lurnip fleff was skening through the dwel",
        "tokens": [
          "A",
          " l",
          "urn",
          "ip",
          " fle",
          "ff",
          " was",
          " sk",
          "ening",
          " through",
          " the",
          " dw",
          "el"
        ],
        "num_tokens": 13,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "verm lurnip fleff wuggle sken zink flib dwel",
        "tokens": [
          "ver",
          "m",
          " l",
          "urn",
          "ip",
          " fle",
          "ff",
          " w",
          "uggle",
          " sk",
          "en",
          " z",
          "ink",
          " fl",
          "ib",
          " dw",
          "el"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "vren cleff jembly teff glendle urnix kren rellick",
        "tokens": [
          "v",
          "ren",
          " cle",
          "ff",
          " j",
          "emb",
          "ly",
          " te",
          "ff",
          " gl",
          "end",
          "le",
          " ",
          "urn",
          "ix",
          " k",
          "ren",
          " re",
          "ll",
          "ick"
        ],
        "num_tokens": 20,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 24,
    "conditions": {
      "sentence": {
        "text": "The author has written several books successfully",
        "tokens": [
          "The",
          " author",
          " has",
          " written",
          " several",
          " books",
          " successfully"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The quemble has yendle plonk plovs fleffly",
        "tokens": [
          "The",
          " qu",
          "em",
          "ble",
          " has",
          " y",
          "end",
          "le",
          " pl",
          "on",
          "k",
          " pl",
          "ov",
          "s",
          " fle",
          "ff",
          "ly"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "kreb quemble jell yendle plonk plov fleff",
        "tokens": [
          "k",
          "reb",
          " qu",
          "em",
          "ble",
          " j",
          "ell",
          " y",
          "end",
          "le",
          " pl",
          "on",
          "k",
          " pl",
          "ov",
          " fle",
          "ff"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "blicket krendle grint wexin tembril trell blork",
        "tokens": [
          "bl",
          "icket",
          " k",
          "rend",
          "le",
          " gr",
          "int",
          " we",
          "x",
          "in",
          " tem",
          "br",
          "il",
          " tre",
          "ll",
          " bl",
          "ork"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 25,
    "conditions": {
      "sentence": {
        "text": "They could see the mountains clearly from here",
        "tokens": [
          "They",
          " could",
          " see",
          " the",
          " mountains",
          " clearly",
          " from",
          " here"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "sniv could vren the trells urnixly from neffin",
        "tokens": [
          "sn",
          "iv",
          " could",
          " v",
          "ren",
          " the",
          " tre",
          "ll",
          "s",
          " ",
          "urn",
          "ix",
          "ly",
          " from",
          " ne",
          "ff",
          "in"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "sniv whisp vren nellick trell urnix prock neffin",
        "tokens": [
          "sn",
          "iv",
          " wh",
          "isp",
          " v",
          "ren",
          " ne",
          "ll",
          "ick",
          " tre",
          "ll",
          " ",
          "urn",
          "ix",
          " pro",
          "ck",
          " ne",
          "ff",
          "in"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "zell moxen kreb zink shill quorb blork grofl",
        "tokens": [
          "z",
          "ell",
          " m",
          "ox",
          "en",
          " k",
          "reb",
          " z",
          "ink",
          " sh",
          "ill",
          " qu",
          "orb",
          " bl",
          "ork",
          " gro",
          "fl"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 26,
    "conditions": {
      "sentence": {
        "text": "The machine is processing the data quite rapidly",
        "tokens": [
          "The",
          " machine",
          " is",
          " processing",
          " the",
          " data",
          " quite",
          " rapidly"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The veff is verming the prock teff ullickly",
        "tokens": [
          "The",
          " ve",
          "ff",
          " is",
          " ver",
          "ming",
          " the",
          " pro",
          "ck",
          " te",
          "ff",
          " u",
          "ll",
          "ick",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "jrenth veff whell verm jrenth prock teff ullick",
        "tokens": [
          "j",
          "ren",
          "th",
          " ve",
          "ff",
          " whe",
          "ll",
          " ver",
          "m",
          " j",
          "ren",
          "th",
          " pro",
          "ck",
          " te",
          "ff",
          " u",
          "ll",
          "ick"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "zink flurn keff kreb kren lemble plonk creff",
        "tokens": [
          "z",
          "ink",
          " fl",
          "urn",
          " ke",
          "ff",
          " k",
          "reb",
          " k",
          "ren",
          " le",
          "mble",
          " pl",
          "on",
          "k",
          " cre",
          "ff"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 27,
    "conditions": {
      "sentence": {
        "text": "A skilled doctor examined the patient thoroughly",
        "tokens": [
          "A",
          " skilled",
          " doctor",
          " examined",
          " the",
          " patient",
          " thoroughly"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A drived kren blicketed the zemble florply",
        "tokens": [
          "A",
          " dri",
          "ved",
          " k",
          "ren",
          " bl",
          "icket",
          "ed",
          " the",
          " z",
          "em",
          "ble",
          " fl",
          "or",
          "ply"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "zink driv kren blicket pliff zemble florp",
        "tokens": [
          "z",
          "ink",
          " dri",
          "v",
          " k",
          "ren",
          " bl",
          "icket",
          " pl",
          "iff",
          " z",
          "em",
          "ble",
          " fl",
          "orp"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "slenk jembly grint blenk drell freff cleff",
        "tokens": [
          "sl",
          "en",
          "k",
          " j",
          "emb",
          "ly",
          " gr",
          "int",
          " bl",
          "en",
          "k",
          " d",
          "rell",
          " fre",
          "ff",
          " cle",
          "ff"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 28,
    "conditions": {
      "sentence": {
        "text": "The team was celebrating the victory enthusiastically",
        "tokens": [
          "The",
          " team",
          " was",
          " celebrating",
          " the",
          " victory",
          " enthusiastically"
        ],
        "num_tokens": 7,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The tembril was krening the dwel drellly",
        "tokens": [
          "The",
          " tem",
          "br",
          "il",
          " was",
          " k",
          "re",
          "ning",
          " the",
          " dw",
          "el",
          " d",
          "rell",
          "ly"
        ],
        "num_tokens": 14,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "fremble tembril flenk kren fremble dwel drell",
        "tokens": [
          "f",
          "rem",
          "ble",
          " tem",
          "br",
          "il",
          " fl",
          "en",
          "k",
          " k",
          "ren",
          " fre",
          "mble",
          " dw",
          "el",
          " d",
          "rell"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "vork yorbel daxen florp glorp tren blundle",
        "tokens": [
          "v",
          "ork",
          " y",
          "or",
          "bel",
          " d",
          "ax",
          "en",
          " fl",
          "orp",
          " gl",
          "orp",
          " t",
          "ren",
          " bl",
          "undle"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 29,
    "conditions": {
      "sentence": {
        "text": "She might have understood the concept more clearly",
        "tokens": [
          "She",
          " might",
          " have",
          " understood",
          " the",
          " concept",
          " more",
          " clearly"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "kren dwel have daffin the kreb flimp yalmly",
        "tokens": [
          "k",
          "ren",
          " dw",
          "el",
          " have",
          " d",
          "aff",
          "in",
          " the",
          " k",
          "reb",
          " fl",
          "imp",
          " y",
          "alm",
          "ly"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "kren dwel trell daffin grell kreb flimp yalm",
        "tokens": [
          "k",
          "ren",
          " dw",
          "el",
          " tre",
          "ll",
          " d",
          "aff",
          "in",
          " gre",
          "ll",
          " k",
          "reb",
          " fl",
          "imp",
          " y",
          "alm"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "hemble slenk klim flenk veff flib bliv blenk",
        "tokens": [
          "hem",
          "ble",
          " sl",
          "en",
          "k",
          " k",
          "lim",
          " fl",
          "en",
          "k",
          " ve",
          "ff",
          " fl",
          "ib",
          " bl",
          "iv",
          " bl",
          "en",
          "k"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 30,
    "conditions": {
      "sentence": {
        "text": "The investors are evaluating the opportunity very carefully",
        "tokens": [
          "The",
          " investors",
          " are",
          " evaluating",
          " the",
          " opportunity",
          " very",
          " carefully"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The wrembles are glendleing the driv very frellly",
        "tokens": [
          "The",
          " wre",
          "mb",
          "les",
          " are",
          " gl",
          "end",
          "le",
          "ing",
          " the",
          " dri",
          "v",
          " very",
          " fre",
          "ll",
          "ly"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "tren wremble gremble glendle tren driv yalm frell",
        "tokens": [
          "t",
          "ren",
          " w",
          "rem",
          "ble",
          " g",
          "rem",
          "ble",
          " gl",
          "end",
          "le",
          " t",
          "ren",
          " dri",
          "v",
          " y",
          "alm",
          " fre",
          "ll"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "reffin veff vork tamp plib cremble plenth bren",
        "tokens": [
          "re",
          "ff",
          "in",
          " ve",
          "ff",
          " v",
          "ork",
          " tamp",
          " pl",
          "ib",
          " cre",
          "mble",
          " pl",
          "enth",
          " b",
          "ren"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  }
]