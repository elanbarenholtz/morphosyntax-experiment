{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locked Design Morphosyntax Audit\n",
    "\n",
    "**Purpose**: Test morphosyntactic constraints across 6 cue families × 6 conditions with integrated context ablation.\n",
    "\n",
    "**Key Features**:\n",
    "- 30 dedicated stimuli per cue family (180 total)\n",
    "- Exactly 1 cue per sentence in controlled position\n",
    "- Context ablation: k ∈ {1, 2, 4, 8, full}\n",
    "- FDR-corrected statistical tests\n",
    "- Publication-ready figures\n",
    "\n",
    "**Cue Families**:\n",
    "1. Infinitival TO → VERB\n",
    "2. Modals → VERB  \n",
    "3. Determiners → NOUN/ADJ\n",
    "4. Prepositions → NP_START\n",
    "5. Auxiliaries → PARTICIPLE\n",
    "6. Complementizers → CLAUSE_START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch pandas matplotlib seaborn scipy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for saving results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set output directory\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/morphosyntax_locked_results'\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Results will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stimulus Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "N_SENTENCES_PER_FAMILY = 30\n",
    "\n",
    "# Function words (for slot identification)\n",
    "FUNCTION_WORDS = {\n",
    "    'the', 'a', 'an', 'this', 'that', 'these', 'those',\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'to', 'at', 'in', 'on', 'with', 'from', 'of', 'for', 'by', 'about',\n",
    "    'and', 'or', 'but', 'so', 'if', 'because', 'that', 'whether',\n",
    "    'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'has', 'have', 'had', 'having', 'do', 'does', 'did',\n",
    "    'will', 'would', 'can', 'could', 'shall', 'should', 'may', 'might', 'must',\n",
    "    'decided', 'wanted', 'began', 'tried', 'planned', 'continued', 'hoped',\n",
    "    'said', 'thought', 'believed', 'knew', 'expected',\n",
    "    'saw', 'heard', 'felt', 'noticed',\n",
    "}\n",
    "\n",
    "# Word pools\n",
    "NOUNS_AGENT = [\n",
    "    'scientist', 'artist', 'teacher', 'student', 'doctor', 'chef', 'engineer',\n",
    "    'musician', 'author', 'mechanic', 'programmer', 'architect', 'researcher',\n",
    "    'photographer', 'detective', 'captain', 'professor', 'coach', 'director',\n",
    "    'farmer', 'baker', 'pilot', 'surgeon', 'lawyer', 'journalist', 'carpenter',\n",
    "    'plumber', 'electrician', 'gardener', 'librarian'\n",
    "]\n",
    "\n",
    "NOUNS_PATIENT = [\n",
    "    'artifacts', 'paintings', 'concept', 'assignment', 'landscape', 'patient',\n",
    "    'symphony', 'novel', 'engine', 'software', 'blueprint', 'data', 'experiment',\n",
    "    'recipe', 'manuscript', 'equipment', 'specimens', 'documents', 'vehicle',\n",
    "    'instrument', 'formula', 'strategy', 'findings', 'technique', 'method',\n",
    "    'discovery', 'solution', 'project', 'design', 'prototype'\n",
    "]\n",
    "\n",
    "ADJECTIVES = [\n",
    "    'ancient', 'beautiful', 'difficult', 'important', 'complex', 'delicious',\n",
    "    'broken', 'efficient', 'innovative', 'comprehensive', 'valuable', 'intricate',\n",
    "    'challenging', 'experimental', 'preliminary', 'historical', 'modern', 'rare',\n",
    "    'elaborate', 'fundamental', 'critical', 'remarkable', 'unusual', 'significant',\n",
    "    'mysterious', 'fascinating', 'practical', 'theoretical', 'advanced', 'basic'\n",
    "]\n",
    "\n",
    "VERBS_BASE = [\n",
    "    'study', 'examine', 'analyze', 'explore', 'investigate', 'review', 'inspect',\n",
    "    'evaluate', 'assess', 'test', 'repair', 'build', 'design', 'create', 'develop',\n",
    "    'improve', 'fix', 'complete', 'finish', 'prepare', 'organize', 'document',\n",
    "    'present', 'explain', 'discuss', 'publish', 'research', 'demonstrate', 'solve', 'process'\n",
    "]\n",
    "\n",
    "VERBS_PAST = [\n",
    "    'decided', 'wanted', 'began', 'tried', 'planned', 'continued', 'hoped',\n",
    "    'expected', 'needed', 'agreed', 'promised', 'attempted', 'managed', 'struggled',\n",
    "    'offered', 'refused', 'learned', 'forgot', 'remembered', 'chose', 'liked',\n",
    "    'loved', 'preferred', 'wished', 'demanded', 'requested', 'intended', 'meant',\n",
    "    'prepared', 'started'\n",
    "]\n",
    "\n",
    "VERBS_PARTICIPLE_ING = [\n",
    "    'studying', 'examining', 'analyzing', 'exploring', 'investigating', 'reviewing',\n",
    "    'inspecting', 'evaluating', 'assessing', 'testing', 'repairing', 'building',\n",
    "    'designing', 'creating', 'developing', 'improving', 'fixing', 'completing',\n",
    "    'finishing', 'preparing', 'organizing', 'documenting', 'presenting', 'explaining',\n",
    "    'discussing', 'publishing', 'researching', 'demonstrating', 'solving', 'processing'\n",
    "]\n",
    "\n",
    "VERBS_PARTICIPLE_ED = [\n",
    "    'studied', 'examined', 'analyzed', 'explored', 'investigated', 'reviewed',\n",
    "    'inspected', 'evaluated', 'assessed', 'tested', 'repaired', 'built',\n",
    "    'designed', 'created', 'developed', 'improved', 'fixed', 'completed',\n",
    "    'finished', 'prepared', 'organized', 'documented', 'presented', 'explained',\n",
    "    'discussed', 'published', 'researched', 'demonstrated', 'solved', 'processed'\n",
    "]\n",
    "\n",
    "PREPOSITIONS_LIST = ['with', 'in', 'on', 'at', 'for', 'about']\n",
    "MODALS_LIST = ['can', 'will', 'could', 'would', 'should', 'must', 'may', 'might']\n",
    "AUXILIARIES_BE = ['is', 'was', 'are', 'were']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonceGenerator:\n",
    "    \"\"\"Generate pronounceable nonce words.\"\"\"\n",
    "    ONSETS = ['b', 'bl', 'br', 'c', 'cl', 'cr', 'd', 'dr', 'f', 'fl', 'fr',\n",
    "              'g', 'gl', 'gr', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'pl', 'pr',\n",
    "              'qu', 'r', 's', 'sc', 'sk', 'sl', 'sm', 'sn', 'sp', 'st', 'str',\n",
    "              'sw', 't', 'tr', 'th', 'v', 'w', 'wh', 'y', 'z']\n",
    "    NUCLEI = ['a', 'e', 'i', 'o', 'u', 'ee', 'oo', 'ea', 'ey']\n",
    "    CODAS = ['', 'b', 'ck', 'd', 'f', 'g', 'k', 'l', 'll', 'm', 'n', 'ng',\n",
    "             'nk', 'p', 'r', 's', 'sk', 'sp', 'ss', 'st', 't', 'x', 'z']\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = random.Random(seed)\n",
    "        self.used = set()\n",
    "\n",
    "    def generate(self) -> str:\n",
    "        for _ in range(100):\n",
    "            word = self.rng.choice(self.ONSETS) + self.rng.choice(self.NUCLEI) + self.rng.choice(self.CODAS)\n",
    "            if word not in FUNCTION_WORDS and word not in self.used and len(word) >= 3:\n",
    "                self.used.add(word)\n",
    "                return word\n",
    "        return f\"zx{self.rng.randint(100, 999)}\"\n",
    "\n",
    "def identify_slots(words):\n",
    "    \"\"\"Identify function vs content slots.\"\"\"\n",
    "    function_slots, content_slots = [], []\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower().strip('.,!?;:') in FUNCTION_WORDS:\n",
    "            function_slots.append((i, word))\n",
    "        else:\n",
    "            content_slots.append((i, word))\n",
    "    return function_slots, content_slots\n",
    "\n",
    "def full_scramble(sentence, seed):\n",
    "    \"\"\"Shuffle ALL words.\"\"\"\n",
    "    words = sentence.split()\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = words.copy()\n",
    "    for _ in range(10):\n",
    "        rng.shuffle(shuffled)\n",
    "        if shuffled != words: break\n",
    "    return ' '.join(shuffled)\n",
    "\n",
    "def content_scramble(sentence, seed):\n",
    "    \"\"\"Shuffle content words among content slots.\"\"\"\n",
    "    words = sentence.split()\n",
    "    func_slots, cont_slots = identify_slots(words)\n",
    "    cont_words = [w for _, w in cont_slots]\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = cont_words.copy()\n",
    "    for _ in range(10):\n",
    "        rng.shuffle(shuffled)\n",
    "        if shuffled != cont_words: break\n",
    "    result = words.copy()\n",
    "    for i, (idx, _) in enumerate(cont_slots):\n",
    "        result[idx] = shuffled[i]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def function_scramble(sentence, seed):\n",
    "    \"\"\"Shuffle function words among function slots.\"\"\"\n",
    "    words = sentence.split()\n",
    "    func_slots, cont_slots = identify_slots(words)\n",
    "    func_words = [w for _, w in func_slots]\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = func_words.copy()\n",
    "    for _ in range(10):\n",
    "        rng.shuffle(shuffled)\n",
    "        if shuffled != func_words: break\n",
    "    result = words.copy()\n",
    "    for i, (idx, _) in enumerate(func_slots):\n",
    "        result[idx] = shuffled[i]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def cue_deletion(sentence, cue_word, replacement='ke'):\n",
    "    \"\"\"Replace cue with nonce.\"\"\"\n",
    "    words = sentence.split()\n",
    "    result = []\n",
    "    found = False\n",
    "    for w in words:\n",
    "        if w.lower().strip('.,!?;:') == cue_word.lower() and not found:\n",
    "            result.append(replacement)\n",
    "            found = True\n",
    "        else:\n",
    "            result.append(w)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_stimuli():\n",
    "    \"\"\"Generate all 180 stimuli (30 per family × 6 families).\"\"\"\n",
    "    rng = random.Random(SEED)\n",
    "    all_stimuli = []\n",
    "    \n",
    "    # Template generators for each family\n",
    "    families = [\n",
    "        ('infinitival_to', 'to', 3, 'VERB'),\n",
    "        ('modals', None, 2, 'VERB'),  # cue varies\n",
    "        ('determiners', 'a', 5, 'NOUN_OR_ADJ'),\n",
    "        ('prepositions', None, 3, 'NP_START'),  # cue varies\n",
    "        ('auxiliaries', None, 2, 'PARTICIPLE'),  # cue varies\n",
    "        ('complementizers', 'that', 3, 'CLAUSE_START'),\n",
    "    ]\n",
    "    \n",
    "    for family, default_cue, cue_pos, target in families:\n",
    "        nonce_gen = NonceGenerator(seed=hash(family) % (2**31))\n",
    "        \n",
    "        agents = rng.sample(NOUNS_AGENT, 30)\n",
    "        patients = rng.sample(NOUNS_PATIENT, 30)\n",
    "        adjs = rng.sample(ADJECTIVES, 30)\n",
    "        \n",
    "        for i in range(N_SENTENCES_PER_FAMILY):\n",
    "            agent = agents[i]\n",
    "            patient = patients[i]\n",
    "            adj = adjs[i]\n",
    "            \n",
    "            # Generate template based on family\n",
    "            if family == 'infinitival_to':\n",
    "                v_past = rng.choice(VERBS_PAST[:15])\n",
    "                v_base = rng.choice(VERBS_BASE)\n",
    "                sentence = f\"the {agent} {v_past} to {v_base} the {adj} {patient}\"\n",
    "                n_agent, n_vbase, n_adj, n_patient = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {v_past} to {n_vbase} the {n_adj} {n_patient}\"\n",
    "                cue = 'to'\n",
    "                \n",
    "            elif family == 'modals':\n",
    "                modal = rng.choice(MODALS_LIST)\n",
    "                v_base = rng.choice(VERBS_BASE)\n",
    "                sentence = f\"the {agent} {modal} {v_base} the {adj} {patient}\"\n",
    "                n_agent, n_vbase, n_adj, n_patient = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {modal} {n_vbase} the {n_adj} {n_patient}\"\n",
    "                cue = modal\n",
    "                \n",
    "            elif family == 'determiners':\n",
    "                v_past = rng.choice(VERBS_PAST)\n",
    "                sentence = f\"the {agent} {v_past} and saw a {adj} {patient}\"\n",
    "                n_agent, n_vpast, n_adj, n_patient = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {n_vpast} and saw a {n_adj} {n_patient}\"\n",
    "                cue = 'a'\n",
    "                \n",
    "            elif family == 'prepositions':\n",
    "                v_past = rng.choice(VERBS_PAST)\n",
    "                prep = rng.choice(PREPOSITIONS_LIST)\n",
    "                sentence = f\"the {agent} {v_past} {prep} the {adj} {patient}\"\n",
    "                n_agent, n_vpast, n_adj, n_patient = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {n_vpast} {prep} the {n_adj} {n_patient}\"\n",
    "                cue = prep\n",
    "                \n",
    "            elif family == 'auxiliaries':\n",
    "                aux = rng.choice(AUXILIARIES_BE)\n",
    "                if i % 2 == 0:\n",
    "                    v_part = rng.choice(VERBS_PARTICIPLE_ING)\n",
    "                else:\n",
    "                    v_part = rng.choice(VERBS_PARTICIPLE_ED)\n",
    "                sentence = f\"the {agent} {aux} {v_part} the {adj} {patient}\"\n",
    "                n_agent, n_vpart, n_adj, n_patient = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {aux} {n_vpart} the {n_adj} {n_patient}\"\n",
    "                cue = aux\n",
    "                \n",
    "            elif family == 'complementizers':\n",
    "                v1 = rng.choice(['said', 'thought', 'believed', 'knew', 'expected', 'hoped'])\n",
    "                v2 = rng.choice(VERBS_PAST)\n",
    "                agent2 = rng.choice(NOUNS_AGENT)\n",
    "                sentence = f\"the {agent} {v1} that the {agent2} {v2}\"\n",
    "                n_agent, n_v1, n_agent2, n_v2 = [nonce_gen.generate() for _ in range(4)]\n",
    "                jabberwocky = f\"the {n_agent} {n_v1} that the {n_agent2} {n_v2}\"\n",
    "                cue = 'that'\n",
    "            \n",
    "            # Generate seeds for scrambles\n",
    "            seed_base = hash(f\"{family}_{i}\") % (2**31)\n",
    "            \n",
    "            # Generate all conditions\n",
    "            stimulus = {\n",
    "                'set_id': i + 1,\n",
    "                'cue_family': family,\n",
    "                'cue_word': cue,\n",
    "                'cue_position': cue_pos,\n",
    "                'target_class': target,\n",
    "                'sentence': sentence,\n",
    "                'jabberwocky': jabberwocky,\n",
    "                'full_scrambled': full_scramble(jabberwocky, seed_base + 1),\n",
    "                'content_scrambled': content_scramble(jabberwocky, seed_base + 2),\n",
    "                'function_scrambled': function_scramble(jabberwocky, seed_base + 3),\n",
    "                'cue_deleted': cue_deletion(jabberwocky, cue),\n",
    "            }\n",
    "            all_stimuli.append(stimulus)\n",
    "    \n",
    "    return all_stimuli\n",
    "\n",
    "# Generate stimuli\n",
    "print(\"Generating stimuli...\")\n",
    "stimuli = generate_all_stimuli()\n",
    "print(f\"Generated {len(stimuli)} stimuli\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExamples from each family:\")\n",
    "for family in ['infinitival_to', 'modals', 'determiners', 'prepositions', 'auxiliaries', 'complementizers']:\n",
    "    s = [x for x in stimuli if x['cue_family'] == family][0]\n",
    "    print(f\"\\n{family} (cue='{s['cue_word']}'):\")\n",
    "    print(f\"  SENTENCE:     {s['sentence']}\")\n",
    "    print(f\"  JABBERWOCKY:  {s['jabberwocky']}\")\n",
    "    print(f\"  FUNC_SCRAMB:  {s['function_scrambled']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word sets for each target class\n",
    "VERB_SET = {\n",
    "    'be', 'have', 'do', 'say', 'go', 'get', 'make', 'know', 'think', 'take',\n",
    "    'see', 'come', 'want', 'use', 'find', 'give', 'tell', 'work', 'call', 'try',\n",
    "    'ask', 'need', 'feel', 'become', 'leave', 'put', 'mean', 'keep', 'let', 'begin',\n",
    "    'seem', 'help', 'show', 'hear', 'play', 'run', 'move', 'live', 'believe',\n",
    "    'bring', 'happen', 'write', 'sit', 'stand', 'lose', 'pay', 'meet', 'continue',\n",
    "    'set', 'learn', 'change', 'lead', 'understand', 'watch', 'follow', 'stop', 'create', 'speak',\n",
    "    'read', 'allow', 'add', 'spend', 'grow', 'open', 'walk', 'win', 'teach', 'offer',\n",
    "    'remember', 'love', 'consider', 'appear', 'buy', 'serve', 'die', 'send', 'build', 'stay',\n",
    "    'fall', 'cut', 'reach', 'kill', 'raise', 'pass', 'sell', 'decide', 'return', 'explain',\n",
    "    'hope', 'develop', 'carry', 'break', 'receive', 'agree', 'support', 'hit', 'produce', 'eat',\n",
    "    'study', 'research', 'investigate', 'examine', 'analyze', 'explore',\n",
    "    'paint', 'draw', 'design', 'construct', 'perform', 'practice',\n",
    "    'publish', 'edit', 'revise', 'prepare', 'cook', 'repair', 'fix',\n",
    "    'solve', 'calculate', 'improve', 'enhance', 'test', 'validate',\n",
    "    'organize', 'arrange', 'defend', 'protect', 'film', 'record',\n",
    "    'sail', 'navigate', 'discuss', 'debate', 'assemble', 'combine',\n",
    "    'plan', 'schedule', 'finish', 'complete', 'start', 'end',\n",
    "}\n",
    "\n",
    "NOUN_SET = {\n",
    "    'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world', 'life', 'hand',\n",
    "    'part', 'child', 'eye', 'woman', 'place', 'work', 'week', 'case', 'point',\n",
    "    'company', 'number', 'group', 'problem', 'fact', 'house', 'area', 'money', 'story', 'student',\n",
    "    'word', 'family', 'head', 'water', 'room', 'mother', 'night', 'home', 'side',\n",
    "    'power', 'hour', 'game', 'line', 'end', 'member', 'law', 'car', 'city',\n",
    "    'name', 'team', 'minute', 'idea', 'body', 'back', 'parent', 'face',\n",
    "    'level', 'office', 'door', 'health', 'art', 'war', 'history', 'result', 'change',\n",
    "    'morning', 'reason', 'research', 'girl', 'guy', 'moment', 'air', 'teacher', 'force',\n",
    "    'scientist', 'artist', 'musician', 'author', 'chef', 'mechanic', 'programmer',\n",
    "    'engineer', 'architect', 'researcher', 'doctor', 'patient', 'professor',\n",
    "    'artifacts', 'paintings', 'symphony', 'novel', 'software', 'blueprint', 'data',\n",
    "    'recipe', 'manuscript', 'equipment', 'documents', 'vehicle', 'experiment',\n",
    "}\n",
    "\n",
    "ADJECTIVE_SET = {\n",
    "    'new', 'good', 'first', 'last', 'long', 'great', 'little', 'own', 'other', 'old',\n",
    "    'right', 'big', 'high', 'different', 'small', 'large', 'next', 'early', 'young', 'important',\n",
    "    'few', 'public', 'bad', 'same', 'able', 'best', 'full', 'simple', 'left', 'late',\n",
    "    'hard', 'real', 'top', 'whole', 'sure', 'better', 'free', 'special', 'clear', 'recent',\n",
    "    'beautiful', 'strong', 'certain', 'open', 'red', 'difficult', 'available', 'likely',\n",
    "    'short', 'single', 'current', 'wrong', 'past', 'fine', 'common', 'poor', 'natural',\n",
    "    'significant', 'similar', 'hot', 'dead', 'happy', 'serious', 'ready', 'easy', 'effective',\n",
    "    'ancient', 'complex', 'delicious', 'broken', 'efficient', 'comprehensive', 'historical',\n",
    "    'rare', 'valuable', 'intricate', 'challenging', 'innovative', 'experimental',\n",
    "}\n",
    "\n",
    "PARTICIPLE_SET = {\n",
    "    'being', 'having', 'doing', 'saying', 'going', 'getting', 'making', 'knowing',\n",
    "    'studying', 'examining', 'analyzing', 'exploring', 'investigating', 'reviewing',\n",
    "    'evaluating', 'testing', 'repairing', 'building', 'designing', 'creating',\n",
    "    'developing', 'improving', 'fixing', 'completing', 'preparing', 'organizing',\n",
    "    'presenting', 'explaining', 'discussing', 'publishing', 'researching', 'solving',\n",
    "    'working', 'running', 'walking', 'talking', 'reading', 'writing', 'playing',\n",
    "    'watching', 'listening', 'thinking', 'looking', 'waiting', 'trying', 'helping',\n",
    "    'been', 'had', 'done', 'said', 'gone', 'made', 'known', 'thought', 'taken',\n",
    "    'seen', 'come', 'found', 'given', 'told', 'worked', 'called', 'tried',\n",
    "    'studied', 'examined', 'analyzed', 'explored', 'investigated', 'reviewed',\n",
    "    'evaluated', 'tested', 'repaired', 'built', 'designed', 'created',\n",
    "    'developed', 'improved', 'fixed', 'completed', 'prepared', 'organized',\n",
    "    'presented', 'explained', 'discussed', 'published', 'researched', 'solved',\n",
    "    'broken', 'written', 'shown', 'chosen', 'spoken', 'frozen',\n",
    "}\n",
    "\n",
    "NP_START_SET = NOUN_SET | ADJECTIVE_SET | {\n",
    "    'the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her',\n",
    "    'its', 'our', 'their', 'some', 'any', 'each', 'every', 'all', 'both', 'many',\n",
    "    'few', 'several', 'other', 'another', 'no', 'one', 'two', 'three',\n",
    "}\n",
    "\n",
    "CLAUSE_START_SET = {\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
    "    'this', 'that', 'these', 'those', 'who', 'what', 'which', 'where', 'when', 'why', 'how',\n",
    "    'the', 'a', 'an', 'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "    'some', 'any', 'all', 'both', 'each', 'every', 'no', 'everyone', 'someone',\n",
    "} | NOUN_SET\n",
    "\n",
    "TARGET_CLASSES = {\n",
    "    'infinitival_to': {'primary': 'VERB', 'word_sets': {'VERB': VERB_SET}},\n",
    "    'modals': {'primary': 'VERB', 'word_sets': {'VERB': VERB_SET}},\n",
    "    'determiners': {'primary': 'NOUN_OR_ADJ', 'word_sets': {'NOUN': NOUN_SET, 'ADJ': ADJECTIVE_SET}},\n",
    "    'prepositions': {'primary': 'NP_START', 'word_sets': {'NP_START': NP_START_SET}},\n",
    "    'auxiliaries': {'primary': 'PARTICIPLE', 'word_sets': {'PARTICIPLE': PARTICIPLE_SET}},\n",
    "    'complementizers': {'primary': 'CLAUSE_START', 'word_sets': {'CLAUSE_START': CLAUSE_START_SET}},\n",
    "}\n",
    "\n",
    "print(\"Target classes defined:\")\n",
    "for family, config in TARGET_CLASSES.items():\n",
    "    n_words = sum(len(ws) for ws in config['word_sets'].values())\n",
    "    print(f\"  {family}: {config['primary']} ({n_words} words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word-Level Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLevelAnalyzer:\n",
    "    \"\"\"Analyze predictions at word level (avoiding BPE artifacts).\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._cache = {}\n",
    "    \n",
    "    def is_word_start_token(self, token_id: int) -> bool:\n",
    "        \"\"\"Check if token is word-start (space-prefixed in GPT-2/Pythia).\"\"\"\n",
    "        if token_id in self._cache:\n",
    "            return self._cache[token_id][0]\n",
    "        \n",
    "        token_str = self.tokenizer.decode([token_id])\n",
    "        if token_str in ['<|endoftext|>', '<unk>', '<pad>', '']:\n",
    "            self._cache[token_id] = (False, None)\n",
    "            return False\n",
    "        \n",
    "        is_start = token_str.startswith(' ') or token_str.startswith('\\n')\n",
    "        word = token_str.strip().lower().strip('.,!?;:\"\\'') if is_start else None\n",
    "        self._cache[token_id] = (is_start, word)\n",
    "        return is_start\n",
    "    \n",
    "    def get_word_from_token(self, token_id: int) -> Optional[str]:\n",
    "        if token_id not in self._cache:\n",
    "            self.is_word_start_token(token_id)\n",
    "        return self._cache[token_id][1]\n",
    "    \n",
    "    def compute_class_mass(self, probs: torch.Tensor, word_sets: Dict[str, Set[str]], top_k: int = 1000) -> Dict[str, float]:\n",
    "        \"\"\"Compute probability mass for each word class.\"\"\"\n",
    "        top_k_probs, top_k_ids = torch.topk(probs, min(top_k, len(probs)))\n",
    "        class_mass = {name: 0.0 for name in word_sets}\n",
    "        \n",
    "        for prob, token_id in zip(top_k_probs, top_k_ids):\n",
    "            word = self.get_word_from_token(token_id.item())\n",
    "            if word is None:\n",
    "                continue\n",
    "            for class_name, word_set in word_sets.items():\n",
    "                if word in word_set:\n",
    "                    class_mass[class_name] += prob.item()\n",
    "        \n",
    "        return class_mass\n",
    "\n",
    "def truncate_context(text: str, cue_position: int, k: int) -> str:\n",
    "    \"\"\"Get last k words up to and including cue.\"\"\"\n",
    "    words = text.split()\n",
    "    if k >= cue_position + 1:\n",
    "        return ' '.join(words[:cue_position + 1])\n",
    "    else:\n",
    "        start = max(0, cue_position + 1 - k)\n",
    "        return ' '.join(words[start:cue_position + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "MODEL_NAME = 'gpt2'  # Options: 'gpt2', 'gpt2-medium', 'gpt2-large', 'EleutherAI/pythia-410m'\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run audit with context ablation\n",
    "CONDITIONS = ['sentence', 'jabberwocky', 'full_scrambled', 'content_scrambled', 'function_scrambled', 'cue_deleted']\n",
    "CONTEXT_LENGTHS = [1, 2, 4, 8, -1]  # -1 = full\n",
    "TOP_K = 1000\n",
    "\n",
    "analyzer = WordLevelAnalyzer(tokenizer)\n",
    "results = []\n",
    "\n",
    "total = len(stimuli) * len(CONDITIONS) * len(CONTEXT_LENGTHS)\n",
    "print(f\"Running audit ({total} iterations)...\")\n",
    "\n",
    "with tqdm(total=total) as pbar:\n",
    "    for stim in stimuli:\n",
    "        family = stim['cue_family']\n",
    "        cue_pos = stim['cue_position']\n",
    "        word_sets = TARGET_CLASSES[family]['word_sets']\n",
    "        \n",
    "        for cond in CONDITIONS:\n",
    "            text = stim[cond]\n",
    "            \n",
    "            for k in CONTEXT_LENGTHS:\n",
    "                # Get context\n",
    "                if k == -1:\n",
    "                    context = ' '.join(text.split()[:cue_pos + 1])\n",
    "                    k_label = 'full'\n",
    "                else:\n",
    "                    context = truncate_context(text, cue_pos, k)\n",
    "                    k_label = str(k)\n",
    "                \n",
    "                # Get predictions\n",
    "                inputs = tokenizer(context, return_tensors='pt').to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                \n",
    "                logits = outputs.logits[0, -1, :]\n",
    "                probs = torch.softmax(logits, dim=-1).cpu()\n",
    "                \n",
    "                # Compute class mass\n",
    "                class_mass = analyzer.compute_class_mass(probs, word_sets, top_k=TOP_K)\n",
    "                target_mass = sum(class_mass.values())\n",
    "                \n",
    "                results.append({\n",
    "                    'set_id': stim['set_id'],\n",
    "                    'cue_family': family,\n",
    "                    'cue_word': stim['cue_word'],\n",
    "                    'condition': cond.upper(),\n",
    "                    'context_k': k_label,\n",
    "                    'target_mass': target_mass,\n",
    "                    'class_mass': class_mass,\n",
    "                })\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "print(f\"\\nAudit complete! {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Show summary\n",
    "print(\"Target mass by family × condition (k=full):\")\n",
    "summary = df[df['context_k'] == 'full'].pivot_table(\n",
    "    values='target_mass', \n",
    "    index='cue_family', \n",
    "    columns='condition',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(summary.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_correction(p_values, alpha=0.05):\n",
    "    \"\"\"Benjamini-Hochberg FDR correction.\"\"\"\n",
    "    n = len(p_values)\n",
    "    sorted_idx = np.argsort(p_values)\n",
    "    sorted_p = p_values[sorted_idx]\n",
    "    adjusted = np.zeros(n)\n",
    "    for i, p in enumerate(sorted_p):\n",
    "        adjusted[i] = p * n / (i + 1)\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        adjusted[i] = min(adjusted[i], adjusted[i + 1])\n",
    "    adjusted = np.minimum(adjusted, 1.0)\n",
    "    result = np.zeros(n)\n",
    "    result[sorted_idx] = adjusted\n",
    "    return result, result < alpha\n",
    "\n",
    "def compute_contrasts(df, context_k='full'):\n",
    "    \"\"\"Compute key paired contrasts.\"\"\"\n",
    "    df_k = df[df['context_k'] == context_k]\n",
    "    \n",
    "    contrasts = [\n",
    "        ('SENTENCE', 'JABBERWOCKY'),\n",
    "        ('JABBERWOCKY', 'FULL_SCRAMBLED'),\n",
    "        ('JABBERWOCKY', 'CONTENT_SCRAMBLED'),\n",
    "        ('JABBERWOCKY', 'FUNCTION_SCRAMBLED'),\n",
    "        ('JABBERWOCKY', 'CUE_DELETED'),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for family in df_k['cue_family'].unique():\n",
    "        df_fam = df_k[df_k['cue_family'] == family]\n",
    "        \n",
    "        for cond_a, cond_b in contrasts:\n",
    "            df_a = df_fam[df_fam['condition'] == cond_a].set_index('set_id')['target_mass']\n",
    "            df_b = df_fam[df_fam['condition'] == cond_b].set_index('set_id')['target_mass']\n",
    "            common = df_a.index.intersection(df_b.index)\n",
    "            \n",
    "            if len(common) == 0:\n",
    "                continue\n",
    "            \n",
    "            x, y = df_a.loc[common].values, df_b.loc[common].values\n",
    "            diff = np.mean(x) - np.mean(y)\n",
    "            t_stat, p_val = stats.ttest_rel(x, y)\n",
    "            d = np.mean(x - y) / np.std(x - y, ddof=1) if np.std(x - y, ddof=1) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'cue_family': family,\n",
    "                'contrast': f\"{cond_a} - {cond_b}\",\n",
    "                'mean_a': np.mean(x),\n",
    "                'mean_b': np.mean(y),\n",
    "                'diff': diff,\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_val,\n",
    "                'cohens_d': d,\n",
    "                'n': len(common),\n",
    "            })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    if len(df_results) > 0:\n",
    "        p_adj, sig = fdr_correction(df_results['p_value'].values)\n",
    "        df_results['p_adjusted'] = p_adj\n",
    "        df_results['significant'] = sig\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Compute contrasts\n",
    "contrasts_df = compute_contrasts(df)\n",
    "print(\"Key contrasts (FDR-corrected):\")\n",
    "print(contrasts_df[['cue_family', 'contrast', 'diff', 'p_adjusted', 'cohens_d', 'significant']].round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure settings\n",
    "plt.rcParams.update({'font.size': 10, 'figure.dpi': 150})\n",
    "\n",
    "CONDITION_COLORS = {\n",
    "    'SENTENCE': '#2ecc71',\n",
    "    'JABBERWOCKY': '#3498db',\n",
    "    'FULL_SCRAMBLED': '#e74c3c',\n",
    "    'CONTENT_SCRAMBLED': '#f39c12',\n",
    "    'FUNCTION_SCRAMBLED': '#9b59b6',\n",
    "    'CUE_DELETED': '#95a5a6',\n",
    "}\n",
    "\n",
    "CONDITION_ORDER = ['SENTENCE', 'JABBERWOCKY', 'FULL_SCRAMBLED', 'CONTENT_SCRAMBLED', 'FUNCTION_SCRAMBLED', 'CUE_DELETED']\n",
    "FAMILY_ORDER = ['infinitival_to', 'modals', 'determiners', 'prepositions', 'auxiliaries', 'complementizers']\n",
    "FAMILY_LABELS = {\n",
    "    'infinitival_to': 'Infinitival TO',\n",
    "    'modals': 'Modals',\n",
    "    'determiners': 'Determiners',\n",
    "    'prepositions': 'Prepositions',\n",
    "    'auxiliaries': 'Auxiliaries',\n",
    "    'complementizers': 'Complementizers',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Slot Constraint by Condition\n",
    "df_full = df[df['context_k'] == 'full']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, family in enumerate(FAMILY_ORDER):\n",
    "    ax = axes[idx]\n",
    "    df_fam = df_full[df_full['cue_family'] == family]\n",
    "    \n",
    "    summary = df_fam.groupby('condition')['target_mass'].agg(['mean', 'std', 'count'])\n",
    "    summary['se'] = summary['std'] / np.sqrt(summary['count'])\n",
    "    \n",
    "    x = np.arange(len(CONDITION_ORDER))\n",
    "    means = [summary.loc[c, 'mean'] if c in summary.index else 0 for c in CONDITION_ORDER]\n",
    "    ses = [summary.loc[c, 'se'] if c in summary.index else 0 for c in CONDITION_ORDER]\n",
    "    colors = [CONDITION_COLORS[c] for c in CONDITION_ORDER]\n",
    "    \n",
    "    ax.bar(x, means, yerr=ses, capsize=3, color=colors, alpha=0.8, edgecolor='white')\n",
    "    ax.set_title(FAMILY_LABELS[family], fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Sent', 'Jab', 'Full', 'Cont', 'Func', 'Cue'], rotation=45, ha='right')\n",
    "    ax.set_ylabel('Target Class Mass')\n",
    "    ax.set_ylim(0, None)\n",
    "\n",
    "plt.suptitle(f'Morphosyntactic Slot Constraint ({MODEL_NAME})', fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/figure1_slot_constraint.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Paired Differences\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "contrasts_to_plot = [\n",
    "    ('JABBERWOCKY', 'FULL_SCRAMBLED', 'JAB-Full'),\n",
    "    ('JABBERWOCKY', 'CONTENT_SCRAMBLED', 'JAB-Cont'),\n",
    "    ('JABBERWOCKY', 'FUNCTION_SCRAMBLED', 'JAB-Func'),\n",
    "]\n",
    "\n",
    "for idx, family in enumerate(FAMILY_ORDER):\n",
    "    ax = axes[idx]\n",
    "    df_fam = df_full[df_full['cue_family'] == family]\n",
    "    \n",
    "    diff_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for cond_a, cond_b, label in contrasts_to_plot:\n",
    "        df_a = df_fam[df_fam['condition'] == cond_a].set_index('set_id')['target_mass']\n",
    "        df_b = df_fam[df_fam['condition'] == cond_b].set_index('set_id')['target_mass']\n",
    "        common = df_a.index.intersection(df_b.index)\n",
    "        if len(common) > 0:\n",
    "            diffs = (df_a.loc[common] - df_b.loc[common]).values\n",
    "            diff_data.append(diffs)\n",
    "            labels.append(label)\n",
    "    \n",
    "    if diff_data:\n",
    "        parts = ax.violinplot(diff_data, showmeans=True)\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(['#3498db', '#f39c12', '#9b59b6'][i])\n",
    "            pc.set_alpha(0.7)\n",
    "        ax.axhline(y=0, color='red', linestyle='--', linewidth=1.5)\n",
    "        ax.set_xticks(range(1, len(labels) + 1))\n",
    "        ax.set_xticklabels(labels)\n",
    "    \n",
    "    ax.set_title(FAMILY_LABELS[family], fontweight='bold')\n",
    "    ax.set_ylabel('Difference')\n",
    "\n",
    "plt.suptitle('Paired Differences (Jabberwocky vs Scrambled)', fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/figure2_paired_differences.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Context Ablation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "k_order = ['1', '2', '4', '8', 'full']\n",
    "k_numeric = {'1': 1, '2': 2, '4': 4, '8': 8, 'full': 16}\n",
    "ablation_conds = ['SENTENCE', 'JABBERWOCKY', 'FULL_SCRAMBLED', 'FUNCTION_SCRAMBLED']\n",
    "\n",
    "for idx, family in enumerate(FAMILY_ORDER):\n",
    "    ax = axes[idx]\n",
    "    df_fam = df[df['cue_family'] == family]\n",
    "    \n",
    "    for cond in ablation_conds:\n",
    "        df_cond = df_fam[df_fam['condition'] == cond]\n",
    "        x_vals, y_vals, y_errs = [], [], []\n",
    "        \n",
    "        for k in k_order:\n",
    "            df_k = df_cond[df_cond['context_k'] == k]\n",
    "            if len(df_k) > 0:\n",
    "                x_vals.append(k_numeric[k])\n",
    "                y_vals.append(df_k['target_mass'].mean())\n",
    "                y_errs.append(df_k['target_mass'].std() / np.sqrt(len(df_k)))\n",
    "        \n",
    "        if x_vals:\n",
    "            ax.errorbar(x_vals, y_vals, yerr=y_errs, marker='o',\n",
    "                       label=cond.replace('_', ' ').title() if idx == 0 else '',\n",
    "                       color=CONDITION_COLORS[cond], capsize=3, linewidth=2)\n",
    "    \n",
    "    ax.set_title(FAMILY_LABELS[family], fontweight='bold')\n",
    "    ax.set_xlabel('Context Length (k)')\n",
    "    ax.set_ylabel('Target Class Mass')\n",
    "    ax.set_xscale('log', base=2)\n",
    "    ax.set_xticks([1, 2, 4, 8, 16])\n",
    "    ax.set_xticklabels(['1', '2', '4', '8', 'full'])\n",
    "\n",
    "fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.02), ncol=4)\n",
    "plt.suptitle('Context Ablation', fontweight='bold', y=1.08)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/figure3_context_ablation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SAVE ALL RESULTS TO CSV AND JSON\n# ============================================================\n\nmodel_slug = MODEL_NAME.replace('/', '_')\nprint(f\"Saving results for model: {MODEL_NAME}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(\"=\" * 60)\n\n# 1. SAVE RAW RESULTS AS JSON (complete with class_mass dict)\nresults_json_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_raw.json'\nwith open(results_json_file, 'w') as f:\n    json.dump({\n        'metadata': {\n            'model': MODEL_NAME,\n            'timestamp': datetime.now().isoformat(),\n            'n_stimuli': len(stimuli),\n            'n_results': len(results),\n            'conditions': CONDITIONS,\n            'context_lengths': CONTEXT_LENGTHS,\n        },\n        'results': results\n    }, f, indent=2)\nprint(f\"[1] Raw results (JSON): {results_json_file}\")\n\n# 2. SAVE RAW RESULTS AS CSV (flattened - one row per observation)\n# Flatten class_mass dict into columns\nresults_flat = []\nfor r in results:\n    row = {\n        'set_id': r['set_id'],\n        'cue_family': r['cue_family'],\n        'cue_word': r['cue_word'],\n        'condition': r['condition'],\n        'context_k': r['context_k'],\n        'target_mass': r['target_mass'],\n    }\n    # Add class_mass as separate columns\n    for class_name, mass in r['class_mass'].items():\n        row[f'mass_{class_name}'] = mass\n    results_flat.append(row)\n\ndf_flat = pd.DataFrame(results_flat)\nresults_csv_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_raw.csv'\ndf_flat.to_csv(results_csv_file, index=False)\nprint(f\"[2] Raw results (CSV):  {results_csv_file}\")\nprint(f\"    Shape: {df_flat.shape[0]} rows × {df_flat.shape[1]} columns\")\n\n# 3. SAVE SUMMARY TABLE (mean target_mass by family × condition, k=full)\nsummary_pivot = df[df['context_k'] == 'full'].pivot_table(\n    values='target_mass', \n    index='cue_family', \n    columns='condition',\n    aggfunc='mean'\n)\nsummary_csv_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_summary.csv'\nsummary_pivot.to_csv(summary_csv_file)\nprint(f\"[3] Summary table (CSV): {summary_csv_file}\")\n\n# Also save with std and n\nsummary_detailed = df[df['context_k'] == 'full'].groupby(['cue_family', 'condition'])['target_mass'].agg(\n    ['mean', 'std', 'count']\n).reset_index()\nsummary_detailed['se'] = summary_detailed['std'] / np.sqrt(summary_detailed['count'])\nsummary_detailed_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_summary_detailed.csv'\nsummary_detailed.to_csv(summary_detailed_file, index=False)\nprint(f\"[4] Summary detailed (CSV): {summary_detailed_file}\")\n\n# 4. SAVE STATISTICAL CONTRASTS\ncontrasts_csv_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_contrasts.csv'\ncontrasts_df.to_csv(contrasts_csv_file, index=False)\nprint(f\"[5] Statistical contrasts (CSV): {contrasts_csv_file}\")\n\n# 5. SAVE CONTEXT ABLATION SUMMARY (JABBERWOCKY by context_k)\nablation_jab = df[df['condition'] == 'JABBERWOCKY'].pivot_table(\n    values='target_mass', \n    index='cue_family', \n    columns='context_k', \n    aggfunc='mean'\n)\n# Reorder columns\ncol_order = ['1', '2', '4', '8', 'full']\nablation_jab = ablation_jab[[c for c in col_order if c in ablation_jab.columns]]\nablation_csv_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_ablation.csv'\nablation_jab.to_csv(ablation_csv_file)\nprint(f\"[6] Context ablation (CSV): {ablation_csv_file}\")\n\n# 6. SAVE ALL CONDITIONS ABLATION\nablation_all = df.pivot_table(\n    values='target_mass',\n    index=['cue_family', 'condition'],\n    columns='context_k',\n    aggfunc='mean'\n)\nablation_all = ablation_all[[c for c in col_order if c in ablation_all.columns]]\nablation_all_file = f'{OUTPUT_DIR}/locked_audit_{model_slug}_ablation_all_conditions.csv'\nablation_all.to_csv(ablation_all_file)\nprint(f\"[7] Ablation all conditions (CSV): {ablation_all_file}\")\n\n# 7. SAVE STIMULI\nstimuli_json_file = f'{OUTPUT_DIR}/stimuli_locked.json'\nwith open(stimuli_json_file, 'w') as f:\n    json.dump(stimuli, f, indent=2)\nprint(f\"[8] Stimuli (JSON): {stimuli_json_file}\")\n\n# Also save stimuli as CSV\nstimuli_df = pd.DataFrame(stimuli)\nstimuli_csv_file = f'{OUTPUT_DIR}/stimuli_locked.csv'\nstimuli_df.to_csv(stimuli_csv_file, index=False)\nprint(f\"[9] Stimuli (CSV): {stimuli_csv_file}\")\n\nprint()\nprint(\"=\" * 60)\nprint(\"ALL FILES SAVED SUCCESSFULLY\")\nprint(\"=\" * 60)\nprint()\nprint(\"Files in output directory:\")\nimport glob\nfor f in sorted(glob.glob(f'{OUTPUT_DIR}/*')):\n    print(f\"  {f.split('/')[-1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretation Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"INTERPRETATION GUIDE\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Key contrasts to examine:\")\n",
    "print()\n",
    "print(\"1. JABBERWOCKY vs FULL_SCRAMBLED\")\n",
    "print(\"   If JAB >> FULL: Structure matters for morphosyntax\")\n",
    "print()\n",
    "print(\"2. JABBERWOCKY vs FUNCTION_SCRAMBLED\")\n",
    "print(\"   If JAB >> FUNC_S: Function-word skeleton is NECESSARY\")\n",
    "print()\n",
    "print(\"3. JABBERWOCKY vs CONTENT_SCRAMBLED\")\n",
    "print(\"   If JAB ≈ CONT_S (p > 0.05): Content order doesn't matter\")\n",
    "print(\"   → Skeleton is SUFFICIENT\")\n",
    "print()\n",
    "print(\"4. SENTENCE vs JABBERWOCKY\")\n",
    "print(\"   If SENT ≈ JAB: Nonce substitution doesn't hurt\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"Expected pattern for 'skeleton sufficiency':\")\n",
    "print(\"  SENTENCE ≈ JABBERWOCKY ≈ CONTENT_SCRAMBLED >> FUNCTION_SCRAMBLED ≈ FULL_SCRAMBLED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}