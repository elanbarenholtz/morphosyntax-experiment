{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Audit: Category-Level Constraint Analysis\n",
    "\n",
    "This notebook demonstrates that syntactic structure shifts probability mass toward grammatically-appropriate POS categories.\n",
    "\n",
    "**Expected Pattern:**\n",
    "- After \"the\" (determiner) → High % NOUN/ADJ in Sentence & Jabberwocky\n",
    "- After \"the\" in Scrambled → Lower % (structure disrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Stimuli File\n",
    "\n",
    "**IMPORTANT:** Click the folder icon on the left sidebar, then drag and drop your `stimuli_with_scrambled.json` file into the Files area.\n",
    "\n",
    "Or run this cell to upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # Click \"Choose Files\" and select stimuli_with_scrambled.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_pos_tag(word):\n",
    "    \"\"\"Simple rule-based POS tagger\"\"\"\n",
    "    word_lower = word.lower().strip()\n",
    "    \n",
    "    if word_lower in ['the', 'a', 'an']:\n",
    "        return 'DET'\n",
    "    if word_lower in ['in', 'on', 'at', 'to', 'for', 'with', 'from', 'by']:\n",
    "        return 'PREP'\n",
    "    if word_lower in ['i', 'you', 'he', 'she', 'it', 'we', 'they']:\n",
    "        return 'PRON'\n",
    "    if word_lower in ['is', 'are', 'was', 'were', 'has', 'have', 'had']:\n",
    "        return 'AUX'\n",
    "    if word_lower.endswith('ly'):\n",
    "        return 'ADV'\n",
    "    if word_lower.endswith('ing') or word_lower.endswith('ed'):\n",
    "        return 'VERB'\n",
    "    \n",
    "    # Default: assume noun\n",
    "    return 'NOUN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading GPT-2...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "print(\"Loading stimuli...\")\n",
    "with open('stimuli_with_scrambled.json') as f:\n",
    "    stimuli = json.load(f)\n",
    "print(f\"Loaded {len(stimuli)} stimulus sets.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run POS Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cue(model, tokenizer, text, cue_word, expected_pos, k=50):\n",
    "    \"\"\"Analyze predictions after a diagnostic cue\"\"\"\n",
    "    words = text.split()\n",
    "    \n",
    "    # Find cue word\n",
    "    cue_position = None\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        if word.lower().strip('.,!?;:') == cue_word.lower():\n",
    "            cue_position = i\n",
    "            break\n",
    "    \n",
    "    if cue_position is None:\n",
    "        return None\n",
    "    \n",
    "    # Get context\n",
    "    context = ' '.join(words[:cue_position+1])\n",
    "    inputs = tokenizer(context, return_tensors='pt')\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        next_token_logits = outputs.logits[0, -1, :]\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        top_k_probs, top_k_ids = torch.topk(probs, k)\n",
    "    \n",
    "    # Decode and tag\n",
    "    candidates = []\n",
    "    for prob, token_id in zip(top_k_probs, top_k_ids):\n",
    "        token_str = tokenizer.decode([token_id], skip_special_tokens=True).strip()\n",
    "        if token_str:\n",
    "            candidates.append({\n",
    "                'token': token_str,\n",
    "                'prob': prob.item(),\n",
    "                'pos': simple_pos_tag(token_str)\n",
    "            })\n",
    "    \n",
    "    # Compute % in expected categories\n",
    "    total_prob = sum(c['prob'] for c in candidates)\n",
    "    expected_prob = sum(c['prob'] for c in candidates if c['pos'] in expected_pos)\n",
    "    expected_pct = (expected_prob / total_prob * 100) if total_prob > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'cue_position': cue_position,\n",
    "        'cue_word': words[cue_position],\n",
    "        'expected_categories': list(expected_pos),\n",
    "        'expected_pct': expected_pct,\n",
    "        'top_5': [(c['token'], c['pos'], f\"{c['prob']*100:.1f}%\") for c in candidates[:5]]\n",
    "    }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POS AUDIT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Analyze first 5 stimuli for \"the\" → NOUN/ADJ pattern\n",
    "for stim_idx in range(min(5, len(stimuli))):\n",
    "    stim = stimuli[stim_idx]\n",
    "    \n",
    "    print(f\"\\nSTIMULUS {stim_idx + 1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
    "        text = stim[condition]\n",
    "        print(f\"\\n{condition.upper()}:\")\n",
    "        print(f\"  Text: {text[:60]}...\")\n",
    "        \n",
    "        result = analyze_cue(model, tokenizer, text, 'the', {'NOUN', 'ADJ'})\n",
    "        \n",
    "        if result:\n",
    "            print(f\"  Expected categories (NOUN/ADJ): {result['expected_pct']:.1f}%\")\n",
    "            print(f\"  Top-5 predictions:\")\n",
    "            for token, pos, prob in result['top_5']:\n",
    "                print(f\"    - {token:15s} [{pos:6s}] {prob}\")\n",
    "        else:\n",
    "            print(\"  (No 'the' found in this text)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print()\n",
    "print(\"If syntax constrains predictions:\")\n",
    "print(\"  - Sentence & Jabberwocky should show HIGH % for expected categories\")\n",
    "print(\"  - Scrambled should show LOWER % (structure disrupted)\")\n",
    "print()\n",
    "print(\"This demonstrates category-level constraint beyond specific lexical items.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Download Results (Optional)\n",
    "\n",
    "Run this cell to save and download the full analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to JSON\n",
    "results = []\n",
    "\n",
    "for stim_idx, stim in enumerate(stimuli):\n",
    "    stim_results = {'stimulus_idx': stim_idx}\n",
    "    \n",
    "    for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
    "        text = stim[condition]\n",
    "        result = analyze_cue(model, tokenizer, text, 'the', {'NOUN', 'ADJ'})\n",
    "        \n",
    "        if result:\n",
    "            stim_results[condition] = {\n",
    "                'expected_pct': result['expected_pct'],\n",
    "                'top_5': result['top_5']\n",
    "            }\n",
    "    \n",
    "    results.append(stim_results)\n",
    "\n",
    "# Save to file\n",
    "with open('pos_audit_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to pos_audit_results.json\")\n",
    "\n",
    "# Download file\n",
    "from google.colab import files\n",
    "files.download('pos_audit_results.json')\n",
    "print(\"Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute averages across all stimuli\n",
    "sentence_pcts = []\n",
    "jab_pcts = []\n",
    "scrambled_pcts = []\n",
    "\n",
    "for stim in stimuli:\n",
    "    for condition, pcts_list in [('sentence', sentence_pcts), \n",
    "                                   ('jabberwocky_matched', jab_pcts),\n",
    "                                   ('scrambled_jabberwocky', scrambled_pcts)]:\n",
    "        text = stim[condition]\n",
    "        result = analyze_cue(model, tokenizer, text, 'the', {'NOUN', 'ADJ'})\n",
    "        if result:\n",
    "            pcts_list.append(result['expected_pct'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS: % Probability on NOUN/ADJ after 'the'\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(f\"Sentence:              {np.mean(sentence_pcts):.1f}% ± {np.std(sentence_pcts):.1f}%  (n={len(sentence_pcts)})\")\n",
    "print(f\"Jabberwocky (matched): {np.mean(jab_pcts):.1f}% ± {np.std(jab_pcts):.1f}%  (n={len(jab_pcts)})\")\n",
    "print(f\"Scrambled:             {np.mean(scrambled_pcts):.1f}% ± {np.std(scrambled_pcts):.1f}%  (n={len(scrambled_pcts)})\")\n",
    "print()\n",
    "print(f\"Δ (Jabberwocky - Scrambled): {np.mean(jab_pcts) - np.mean(scrambled_pcts):+.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
