[
  {
    "set_id": 1,
    "conditions": {
      "sentence": {
        "text": "The teacher was explaining the concept to the students clearly",
        "tokens": [
          "The",
          " teacher",
          " was",
          " explaining",
          " the",
          " concept",
          " to",
          " the",
          " students",
          " clearly"
        ],
        "num_tokens": 10,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The nellick was trening the trell to the geffs blundlely",
        "tokens": [
          "The",
          " ne",
          "ll",
          "ick",
          " was",
          " tre",
          "ning",
          " the",
          " tre",
          "ll",
          " to",
          " the",
          " ge",
          "ff",
          "s",
          " bl",
          "undle",
          "ly"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "freff nellick keff tren freff trell grofl freff geff blundle",
        "tokens": [
          "fre",
          "ff",
          " ne",
          "ll",
          "ick",
          " ke",
          "ff",
          " t",
          "ren",
          " fre",
          "ff",
          " tre",
          "ll",
          " gro",
          "fl",
          " fre",
          "ff",
          " ge",
          "ff",
          " bl",
          "undle"
        ],
        "num_tokens": 21,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "zemble zrell kren bliv wreffin cremble plenth drell quorb heffin",
        "tokens": [
          "z",
          "em",
          "ble",
          " z",
          "rell",
          " k",
          "ren",
          " bl",
          "iv",
          " wre",
          "ff",
          "in",
          " cre",
          "mble",
          " pl",
          "enth",
          " d",
          "rell",
          " qu",
          "orb",
          " he",
          "ff",
          "in"
        ],
        "num_tokens": 23,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 2,
    "conditions": {
      "sentence": {
        "text": "A small bird landed softly on the wooden fence",
        "tokens": [
          "A",
          " small",
          " bird",
          " landed",
          " softly",
          " on",
          " the",
          " wooden",
          " fence"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "A yendle vork whiffed glorply on the driv moxen",
        "tokens": [
          "A",
          " y",
          "end",
          "le",
          " v",
          "ork",
          " whiff",
          "ed",
          " glor",
          "ply",
          " on",
          " the",
          " dri",
          "v",
          " m",
          "ox",
          "en"
        ],
        "num_tokens": 17,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "vren yendle vork whiff glorp yalm hreff driv moxen",
        "tokens": [
          "v",
          "ren",
          " y",
          "end",
          "le",
          " v",
          "ork",
          " whiff",
          " gl",
          "orp",
          " y",
          "alm",
          " h",
          "re",
          "ff",
          " dri",
          "v",
          " m",
          "ox",
          "en"
        ],
        "num_tokens": 20,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "mornix cromple blenk drell plenth yeff remble trell gribble",
        "tokens": [
          "m",
          "orn",
          "ix",
          " c",
          "rom",
          "ple",
          " bl",
          "en",
          "k",
          " d",
          "rell",
          " pl",
          "enth",
          " ye",
          "ff",
          " rem",
          "ble",
          " tre",
          "ll",
          " g",
          "rib",
          "ble"
        ],
        "num_tokens": 22,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 3,
    "conditions": {
      "sentence": {
        "text": "The children were playing with their toys in the garden",
        "tokens": [
          "The",
          " children",
          " were",
          " playing",
          " with",
          " their",
          " toys",
          " in",
          " the",
          " garden"
        ],
        "num_tokens": 10,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The moxen were mornixing with their slenks in the trock",
        "tokens": [
          "The",
          " m",
          "ox",
          "en",
          " were",
          " m",
          "orn",
          "ix",
          "ing",
          " with",
          " their",
          " sl",
          "en",
          "ks",
          " in",
          " the",
          " tro",
          "ck"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "tembly moxen blenk mornix sniv blemble slenk fleff tembly trock",
        "tokens": [
          "tem",
          "bly",
          " m",
          "ox",
          "en",
          " bl",
          "en",
          "k",
          " m",
          "orn",
          "ix",
          " sn",
          "iv",
          " ble",
          "mble",
          " sl",
          "en",
          "k",
          " fle",
          "ff",
          " tem",
          "bly",
          " tro",
          "ck"
        ],
        "num_tokens": 24,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "heffin flenk demble grenth wreffin yendle frell glendle whill jendle",
        "tokens": [
          "he",
          "ff",
          "in",
          " fl",
          "en",
          "k",
          " dem",
          "ble",
          " gren",
          "th",
          " wre",
          "ff",
          "in",
          " y",
          "end",
          "le",
          " fre",
          "ll",
          " gl",
          "end",
          "le",
          " wh",
          "ill",
          " j",
          "end",
          "le"
        ],
        "num_tokens": 26,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 4,
    "conditions": {
      "sentence": {
        "text": "She has been reading that book very carefully",
        "tokens": [
          "She",
          " has",
          " been",
          " reading",
          " that",
          " book",
          " very",
          " carefully"
        ],
        "num_tokens": 8,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "krendle has been yalming that klim very zinkly",
        "tokens": [
          "k",
          "rend",
          "le",
          " has",
          " been",
          " y",
          "al",
          "ming",
          " that",
          " k",
          "lim",
          " very",
          " z",
          "ink",
          "ly"
        ],
        "num_tokens": 15,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "krendle drell cleff yalm yorbel klim grell zink",
        "tokens": [
          "k",
          "rend",
          "le",
          " d",
          "rell",
          " cle",
          "ff",
          " y",
          "alm",
          " y",
          "or",
          "bel",
          " k",
          "lim",
          " gre",
          "ll",
          " z",
          "ink"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "quorb tren plov nellick fleff wreffin jell snell",
        "tokens": [
          "qu",
          "orb",
          " t",
          "ren",
          " pl",
          "ov",
          " ne",
          "ll",
          "ick",
          " fle",
          "ff",
          " wre",
          "ff",
          "in",
          " j",
          "ell",
          " sn",
          "ell"
        ],
        "num_tokens": 18,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  },
  {
    "set_id": 5,
    "conditions": {
      "sentence": {
        "text": "The scientist discovered a new species in the forest",
        "tokens": [
          "The",
          " scientist",
          " discovered",
          " a",
          " new",
          " species",
          " in",
          " the",
          " forest"
        ],
        "num_tokens": 9,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "jabberwocky": {
        "text": "The sprock ploved a moxen whiffs in the reffin",
        "tokens": [
          "The",
          " sp",
          "rock",
          " pl",
          "oved",
          " a",
          " m",
          "ox",
          "en",
          " wh",
          "iffs",
          " in",
          " the",
          " re",
          "ff",
          "in"
        ],
        "num_tokens": 16,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "stripped": {
        "text": "veff sprock plov tembly moxen whiff pliff veff reffin",
        "tokens": [
          "ve",
          "ff",
          " sp",
          "rock",
          " pl",
          "ov",
          " tem",
          "bly",
          " m",
          "ox",
          "en",
          " whiff",
          " pl",
          "iff",
          " ve",
          "ff",
          " re",
          "ff",
          "in"
        ],
        "num_tokens": 19,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      },
      "nonwords": {
        "text": "freff kren teff grent wreffin sniv jembly grell remble",
        "tokens": [
          "fre",
          "ff",
          " k",
          "ren",
          " te",
          "ff",
          " g",
          "rent",
          " wre",
          "ff",
          "in",
          " sn",
          "iv",
          " j",
          "emb",
          "ly",
          " gre",
          "ll",
          " rem",
          "ble"
        ],
        "num_tokens": 20,
        "mean_entropy": NaN,
        "mean_next_token_prob": NaN,
        "token_entropies": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ],
        "token_probs": [
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN,
          NaN
        ]
      }
    }
  }
]