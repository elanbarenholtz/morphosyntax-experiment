{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Morphosyntactic Constraint Audit\n",
        "\n",
        "**Tests category-level constraint at diagnostic syntactic cues**\n",
        "\n",
        "## Research Question\n",
        "Does syntactic structure shift probability mass toward grammatically-appropriate lexical classes (VERB/NOUN/FUNCTION) at diagnostic cue positions?\n",
        "\n",
        "## Key Improvement over POS Audit\n",
        "- **No POS tagger** â†’ Uses lexicon-based classification\n",
        "- **Word-start only** â†’ Avoids BPE fragment issues\n",
        "- **Multiple cue types** â†’ Tests verb slots, noun slots, NP starts\n",
        "\n",
        "## Cues Tested\n",
        "1. **Infinitival \"to\"** â†’ Expects VERB\n",
        "2. **Modals** (can, will, would, etc.) â†’ Expects VERB\n",
        "3. **Aux/copula** (is, are, was, were) â†’ Expects open-class\n",
        "4. **Prepositions** (in, on, at, etc.) â†’ Expects DET/NOUN\n",
        "\n",
        "## Expected Pattern\n",
        "- **Sentence & Jabberwocky**: High mass on expected class (structure intact)\n",
        "- **Scrambled**: Lower mass (structure disrupted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Stimuli File\n",
        "\n",
        "**IMPORTANT:** Click the **folder icon** (ðŸ“) on the left sidebar, then drag and drop `stimuli_with_scrambled.json` into the Files area.\n",
        "\n",
        "Or run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload stimuli_with_scrambled.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function words (closed-class, ~200 items)\n",
        "FUNCTION_SET = {\n",
        "    # Determiners\n",
        "    'the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his',\n",
        "    'her', 'its', 'our', 'their', 'some', 'any', 'no', 'every', 'each', 'either',\n",
        "    'neither', 'much', 'many', 'more', 'most', 'few', 'several', 'all', 'both',\n",
        "    # Pronouns\n",
        "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "    'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'themselves',\n",
        "    'who', 'whom', 'whose', 'what', 'which', 'whoever', 'whomever', 'whatever',\n",
        "    # Auxiliaries\n",
        "    'is', 'are', 'am', 'was', 'were', 'be', 'been', 'being',\n",
        "    'have', 'has', 'had', 'having',\n",
        "    'do', 'does', 'did', 'doing',\n",
        "    'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would',\n",
        "    # Prepositions\n",
        "    'in', 'on', 'at', 'to', 'for', 'with', 'from', 'by', 'about', 'as', 'of',\n",
        "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between',\n",
        "    'among', 'under', 'over', 'against', 'within', 'without', 'throughout',\n",
        "    # Conjunctions\n",
        "    'and', 'or', 'but', 'nor', 'so', 'yet',\n",
        "    'because', 'since', 'unless', 'until', 'while', 'although', 'though',\n",
        "    'if', 'when', 'where', 'whether', 'than',\n",
        "    # Negation & other\n",
        "    'not', \"n't\", 'there', 'here', 'then', 'now', 'very', 'too', 'also', 'just', 'only',\n",
        "}\n",
        "\n",
        "# Common verbs\n",
        "VERB_SET = {\n",
        "    'be', 'have', 'do', 'say', 'get', 'make', 'go', 'know', 'take', 'see',\n",
        "    'come', 'think', 'look', 'want', 'give', 'use', 'find', 'tell', 'ask',\n",
        "    'work', 'seem', 'feel', 'try', 'leave', 'call', 'become', 'run', 'move',\n",
        "    'live', 'believe', 'bring', 'happen', 'write', 'sit', 'stand', 'lose',\n",
        "    'pay', 'meet', 'include', 'continue', 'set', 'learn', 'change', 'lead',\n",
        "    'understand', 'watch', 'follow', 'stop', 'create', 'speak', 'read', 'allow',\n",
        "    'add', 'spend', 'grow', 'open', 'walk', 'win', 'offer', 'remember', 'love',\n",
        "    'consider', 'appear', 'buy', 'wait', 'serve', 'die', 'send', 'expect',\n",
        "    'build', 'stay', 'fall', 'cut', 'reach', 'kill', 'raise', 'pass', 'sell',\n",
        "    # -ing/-ed forms\n",
        "    'going', 'making', 'doing', 'saying', 'getting', 'taking', 'seeing', 'coming',\n",
        "    'looking', 'working', 'trying', 'running', 'playing', 'showing', 'talking',\n",
        "    'went', 'made', 'said', 'got', 'took', 'saw', 'came', 'looked', 'worked',\n",
        "}\n",
        "\n",
        "# Common nouns\n",
        "NOUN_SET = {\n",
        "    'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world', 'life',\n",
        "    'hand', 'part', 'child', 'eye', 'woman', 'place', 'work', 'week', 'case',\n",
        "    'point', 'government', 'company', 'number', 'group', 'problem', 'fact',\n",
        "    'people', 'water', 'room', 'mother', 'area', 'money', 'story', 'family',\n",
        "    'student', 'word', 'business', 'country', 'question', 'school', 'state',\n",
        "    'night', 'head', 'home', 'office', 'power', 'hour', 'game', 'line', 'end',\n",
        "    'member', 'law', 'car', 'city', 'name', 'team', 'minute', 'idea', 'body',\n",
        "    'dog', 'house', 'president', 'book', 'community', 'computer',\n",
        "}\n",
        "\n",
        "print(f\"âœ“ Lexicons loaded:\")\n",
        "print(f\"  FUNCTION: {len(FUNCTION_SET)} words\")\n",
        "print(f\"  VERB:     {len(VERB_SET)} words\")\n",
        "print(f\"  NOUN:     {len(NOUN_SET)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Define Cue Specifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUE_SPECS = {\n",
        "    'infinitival_to': {\n",
        "        'cues': ['to'],\n",
        "        'expected_class': 'verb',\n",
        "        'description': 'Infinitival \"to\" expects VERB'\n",
        "    },\n",
        "    'modal': {\n",
        "        'cues': ['can', 'will', 'would', 'could', 'should', 'must', 'may', 'might'],\n",
        "        'expected_class': 'verb',\n",
        "        'description': 'Modals expect VERB'\n",
        "    },\n",
        "    'aux_copula': {\n",
        "        'cues': ['is', 'are', 'was', 'were'],\n",
        "        'expected_class': 'open_class',\n",
        "        'description': 'Aux/copula expects open-class'\n",
        "    },\n",
        "    'preposition': {\n",
        "        'cues': ['in', 'on', 'at', 'with', 'from', 'for'],\n",
        "        'expected_class': 'function_or_noun',\n",
        "        'description': 'Prepositions expect DET/NOUN'\n",
        "    },\n",
        "}\n",
        "\n",
        "for cue_type, spec in CUE_SPECS.items():\n",
        "    print(f\"  {cue_type:20s}: {spec['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Classification Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_word_start_token(token_str):\n",
        "    \"\"\"Check if BPE token starts a new word (space + letter).\"\"\"\n",
        "    if not token_str:\n",
        "        return False\n",
        "    if token_str[0] == ' ' and len(token_str) > 1 and token_str[1].isalpha():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def decode_to_word(token_str):\n",
        "    \"\"\"Strip leading space and trailing punctuation to get base word.\"\"\"\n",
        "    word = token_str.strip()\n",
        "    # Strip trailing punctuation\n",
        "    while word and not word[-1].isalnum():\n",
        "        word = word[:-1]\n",
        "    return word.lower()\n",
        "\n",
        "def classify_candidate(word):\n",
        "    \"\"\"\n",
        "    Classify word into lexical class.\n",
        "    Priority: FUNCTION > VERB > NOUN > OTHER\n",
        "    \"\"\"\n",
        "    if not word:\n",
        "        return 'other'\n",
        "\n",
        "    # Check if punctuation only\n",
        "    if all(not c.isalnum() for c in word):\n",
        "        return 'punct'\n",
        "\n",
        "    word_lower = word.lower()\n",
        "\n",
        "    # Priority order\n",
        "    if word_lower in FUNCTION_SET:\n",
        "        return 'function'\n",
        "    if word_lower in VERB_SET:\n",
        "        return 'verb'\n",
        "    if word_lower in NOUN_SET:\n",
        "        return 'noun'\n",
        "\n",
        "    return 'other_open'\n",
        "\n",
        "print(\"âœ“ Classification functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Sanity Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"SANITY CHECKS: Classification\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "test_cases = [\n",
        "    (' the', 'function', 'Determiner â†’ FUNCTION'),\n",
        "    (' and', 'function', 'Conjunction â†’ FUNCTION'),\n",
        "    (' is', 'function', 'Auxiliary â†’ FUNCTION'),\n",
        "    (' run', 'verb', 'Common verb â†’ VERB'),\n",
        "    (' running', 'verb', 'Verb -ing â†’ VERB'),\n",
        "    (' time', 'noun', 'Common noun â†’ NOUN'),\n",
        "    (' people', 'noun', 'Common noun â†’ NOUN'),\n",
        "    (',', 'punct', 'Comma â†’ PUNCT (after decode)'),\n",
        "    ('.', 'punct', 'Period â†’ PUNCT (after decode)'),\n",
        "]\n",
        "\n",
        "all_passed = True\n",
        "for token, expected, desc in test_cases:\n",
        "    word = decode_to_word(token)\n",
        "    result = classify_candidate(word)\n",
        "    status = \"âœ“\" if result == expected else \"âœ— FAIL\"\n",
        "    if result != expected:\n",
        "        all_passed = False\n",
        "    print(f\"{status}  {repr(token):15s} â†’ {word:12s} â†’ {result:12s}  ({desc})\")\n",
        "\n",
        "print()\n",
        "if all_passed:\n",
        "    print(\"âœ“ All sanity checks passed!\")\n",
        "else:\n",
        "    print(\"âœ— Some tests FAILED - check classification logic\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Load Model and Stimuli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"Loading GPT-2...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "print(\"âœ“ Model loaded\\n\")\n",
        "\n",
        "print(\"Loading stimuli...\")\n",
        "with open('stimuli_with_scrambled.json') as f:\n",
        "    stimuli = json.load(f)\n",
        "print(f\"âœ“ Loaded {len(stimuli)} stimulus sets\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Define Analysis Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_cue_position(model, tokenizer, text, cue_word, top_k=5000, debug=False):\n",
        "    \"\"\"\n",
        "    Analyze predictions after a diagnostic cue.\n",
        "\n",
        "    Returns:\n",
        "        dict with:\n",
        "        - mass: probability mass by class\n",
        "        - top_candidates: top-20 word-start candidates (if debug=True)\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "\n",
        "    # Find cue position\n",
        "    cue_position = None\n",
        "    for i, word in enumerate(words[:-1]):\n",
        "        if word.lower().strip('.,!?;:') == cue_word.lower():\n",
        "            cue_position = i\n",
        "            break\n",
        "\n",
        "    if cue_position is None:\n",
        "        return None\n",
        "\n",
        "    # Get context (prefix ending after cue)\n",
        "    context = ' '.join(words[:cue_position+1])\n",
        "    inputs = tokenizer(context, return_tensors='pt')\n",
        "\n",
        "    # Get next-token predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        next_token_logits = outputs.logits[0, -1, :]\n",
        "        probs = torch.softmax(next_token_logits, dim=-1)\n",
        "        top_k_probs, top_k_ids = torch.topk(probs, min(top_k, len(probs)))\n",
        "\n",
        "    # Classify candidates and accumulate mass\n",
        "    mass = {\n",
        "        'verb': 0.0,\n",
        "        'noun': 0.0,\n",
        "        'function': 0.0,\n",
        "        'other_open': 0.0,\n",
        "        'punct': 0.0,\n",
        "        'non_wordstart': 0.0,\n",
        "    }\n",
        "\n",
        "    word_start_candidates = []\n",
        "\n",
        "    for prob, token_id in zip(top_k_probs, top_k_ids):\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "\n",
        "        # Check if word-start\n",
        "        if is_word_start_token(token_str):\n",
        "            word = decode_to_word(token_str)\n",
        "            word_class = classify_candidate(word)\n",
        "\n",
        "            mass[word_class] += prob.item()\n",
        "\n",
        "            if debug and len(word_start_candidates) < 20:\n",
        "                word_start_candidates.append({\n",
        "                    'token': repr(token_str),\n",
        "                    'word': word,\n",
        "                    'class': word_class,\n",
        "                    'prob': prob.item()\n",
        "                })\n",
        "        else:\n",
        "            mass['non_wordstart'] += prob.item()\n",
        "\n",
        "    # Compute residual (mass not in top-k)\n",
        "    mass['residual'] = 1.0 - sum(mass.values())\n",
        "\n",
        "    result = {\n",
        "        'cue_word_index': cue_position,\n",
        "        'mass': mass,\n",
        "    }\n",
        "\n",
        "    if debug:\n",
        "        result['context'] = context[-60:] if len(context) > 60 else context\n",
        "        result['top_candidates'] = word_start_candidates\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"âœ“ Analysis function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Run Analysis on First 5 Stimuli (Preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"MORPHOSYNTACTIC CONSTRAINT AUDIT - PREVIEW (First 5 Stimuli)\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Run on first cue type only for preview\n",
        "cue_type = 'infinitival_to'\n",
        "spec = CUE_SPECS[cue_type]\n",
        "\n",
        "print(f\"CUE TYPE: {spec['description']}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "for stim_idx in range(min(5, len(stimuli))):\n",
        "    stim = stimuli[stim_idx]\n",
        "    \n",
        "    print(f\"\\nSTIMULUS {stim_idx + 1}:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
        "        text = stim[condition]\n",
        "        \n",
        "        # Try first cue\n",
        "        cue_word = spec['cues'][0]\n",
        "        result = analyze_cue_position(model, tokenizer, text, cue_word, top_k=5000, debug=True)\n",
        "        \n",
        "        if result is not None:\n",
        "            print(f\"\\n{condition.upper()}:\")\n",
        "            print(f\"  Context: ...{result['context']}\")\n",
        "            print(f\"  Mass distribution:\")\n",
        "            for class_name in ['verb', 'noun', 'function', 'other_open', 'punct']:\n",
        "                print(f\"    {class_name:15s}: {result['mass'][class_name]*100:5.1f}%\")\n",
        "            print(f\"  Top-10 word-start candidates:\")\n",
        "            for cand in result.get('top_candidates', [])[:10]:\n",
        "                print(f\"    {cand['token']:20s} [{cand['class']:12s}] {cand['prob']*100:5.1f}%\")\n",
        "        else:\n",
        "            print(f\"\\n{condition.upper()}: (No '{cue_word}' found)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Preview complete. Run full analysis in next cell.\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Run Full Analysis (All Cue Types, All Stimuli)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running full analysis on all cue types and stimuli...\")\n",
        "print(\"(This may take 5-10 minutes)\\n\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# For each cue type\n",
        "for cue_type, spec in CUE_SPECS.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing: {spec['description']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    cue_results = defaultdict(list)\n",
        "    \n",
        "    # Process each stimulus\n",
        "    for stim_idx, stim in enumerate(stimuli):\n",
        "        if (stim_idx + 1) % 10 == 0:\n",
        "            print(f\"  Processing stimulus {stim_idx + 1}/{len(stimuli)}...\")\n",
        "        \n",
        "        for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
        "            text = stim[condition]\n",
        "            \n",
        "            # Try each cue in this spec\n",
        "            for cue_word in spec['cues']:\n",
        "                result = analyze_cue_position(\n",
        "                    model, tokenizer, text, cue_word,\n",
        "                    top_k=5000, debug=False\n",
        "                )\n",
        "                \n",
        "                if result is not None:\n",
        "                    # Store result\n",
        "                    result_record = {\n",
        "                        'model': 'gpt2',\n",
        "                        'cue_type': cue_type,\n",
        "                        'condition': condition,\n",
        "                        'stimulus_id': stim_idx,\n",
        "                        'cue_word': cue_word,\n",
        "                        **result\n",
        "                    }\n",
        "                    \n",
        "                    all_results.append(result_record)\n",
        "                    cue_results[condition].append(result['mass'])\n",
        "    \n",
        "    # Summary for this cue type\n",
        "    print(f\"\\nSUMMARY: {spec['description']}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
        "        if cue_results[condition]:\n",
        "            masses = cue_results[condition]\n",
        "            n = len(masses)\n",
        "            \n",
        "            mean_mass = {\n",
        "                class_name: np.mean([m[class_name] for m in masses])\n",
        "                for class_name in ['verb', 'noun', 'function', 'other_open', 'punct']\n",
        "            }\n",
        "            \n",
        "            print(f\"  {condition:30s} (n={n}):\")\n",
        "            for class_name in ['verb', 'noun', 'function', 'other_open', 'punct']:\n",
        "                print(f\"    {class_name:15s}: {mean_mass[class_name]*100:5.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ“ Full analysis complete!\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Save and Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to JSON\n",
        "output_file = 'morphosyntax_audit_results.json'\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f\"âœ“ Results saved to {output_file}\")\n",
        "print(f\"  Total records: {len(all_results)}\")\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"âœ“ Download started!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Aggregate Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"AGGREGATE SUMMARY: All Cue Types\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Group by cue type and condition\n",
        "for cue_type, spec in CUE_SPECS.items():\n",
        "    print(f\"\\n{spec['description']}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Filter results for this cue type\n",
        "    cue_data = [r for r in all_results if r['cue_type'] == cue_type]\n",
        "    \n",
        "    if not cue_data:\n",
        "        print(\"  (No data)\")\n",
        "        continue\n",
        "    \n",
        "    for condition in ['sentence', 'jabberwocky_matched', 'scrambled_jabberwocky']:\n",
        "        condition_data = [r for r in cue_data if r['condition'] == condition]\n",
        "        \n",
        "        if condition_data:\n",
        "            n = len(condition_data)\n",
        "            \n",
        "            # Compute mean and SEM for each class\n",
        "            stats = {}\n",
        "            for class_name in ['verb', 'noun', 'function', 'other_open']:\n",
        "                values = [r['mass'][class_name] for r in condition_data]\n",
        "                mean = np.mean(values)\n",
        "                sem = np.std(values) / np.sqrt(n)\n",
        "                stats[class_name] = (mean, sem)\n",
        "            \n",
        "            print(f\"\\n  {condition.upper()} (n={n}):\")\n",
        "            for class_name in ['verb', 'noun', 'function', 'other_open']:\n",
        "                mean, sem = stats[class_name]\n",
        "                print(f\"    {class_name:15s}: {mean*100:5.1f}% Â± {sem*100:4.2f}%\")\n",
        "    \n",
        "    # Key contrasts\n",
        "    sent_verb = np.mean([r['mass']['verb'] for r in cue_data if r['condition'] == 'sentence'])\n",
        "    jab_verb = np.mean([r['mass']['verb'] for r in cue_data if r['condition'] == 'jabberwocky_matched'])\n",
        "    scr_verb = np.mean([r['mass']['verb'] for r in cue_data if r['condition'] == 'scrambled_jabberwocky'])\n",
        "    \n",
        "    print(f\"\\n  KEY CONTRASTS:\")\n",
        "    print(f\"    Sentence - Scrambled:     {(sent_verb - scr_verb)*100:+.1f}%\")\n",
        "    print(f\"    Jabberwocky - Scrambled:  {(jab_verb - scr_verb)*100:+.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "### What We're Testing\n",
        "\n",
        "Does syntactic structure shift probability mass toward grammatically-appropriate lexical classes?\n",
        "\n",
        "### Expected Patterns\n",
        "\n",
        "1. **After \"to\" and modals**: \n",
        "   - High VERB mass in Sentence & Jabberwocky\n",
        "   - Lower VERB mass in Scrambled\n",
        "   - â†’ Structure drives verb-slot selectivity\n",
        "\n",
        "2. **After prepositions**:\n",
        "   - Higher FUNCTION + NOUN mass in Sentence & Jabberwocky\n",
        "   - Lower in Scrambled\n",
        "   - â†’ Structure drives NP-start selectivity\n",
        "\n",
        "### What This Tells Us\n",
        "\n",
        "**If we see these patterns**: Entropy reduction corresponds to category-level constraint at diagnostic morphosyntactic cues. The model's narrowed continuation space reflects grammatical structure, not just lexical statistics.\n",
        "\n",
        "**For your paper**: \n",
        "> \"After infinitival *to*, probability mass on verb continuations increased by X% in Jabberwocky relative to Scrambled, demonstrating that morphosyntactic cuesâ€”independent of lexical semanticsâ€”constrain the model's predictions toward grammatically-appropriate word classes.\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
