{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Comprehensive Morphosyntax Constraint Audit\n\n**Tests morphosyntactic knowledge across:**\n- 6 stimulus conditions (SENTENCE, JABBERWOCKY, FULL_SCRAMBLED, CONTENT_SCRAMBLED, FUNCTION_SCRAMBLED, CUE_DELETED)\n- 6 cue families (infinitival_to, modals, determiners, prepositions, auxiliaries, complementizers)\n- Word-level analysis (avoids BPE artifacts)\n\n**Runtime**: ~5-10 minutes per model (with GPU)\n\n**✨ Google Drive Integration**: All results automatically save to Google Drive and persist across disconnections!\n\n**Setup Instructions**:\n1. **Enable GPU**: Runtime → Change runtime type → GPU → Save\n2. **Run all cells**: Runtime → Run all (or Ctrl+F9)\n3. **Authorize Google Drive** (one-time authorization prompt in Cell 2)\n4. **Upload files** if not already in Drive (Cell 4 checks automatically)"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Upload Required Files (One-Time Only)\n\n**Smart Upload**: This cell checks if files already exist in Google Drive. If they do, it skips uploading!\n\n**Required files**:\n1. `cue_families.py`\n2. `word_level_analysis.py`\n3. `run_comprehensive_audit.py`\n4. `analyze_comprehensive_results.py`\n5. `stimuli_comprehensive.json`\n\n**Optional** (for context ablation):\n6. `run_context_ablation.py`\n7. `analyze_context_ablation.py`\n\n**Note**: Files upload to Google Drive, so you only need to do this ONCE. Next session, they'll already be there!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# ============================================================================\n# Check for Required Files & Upload if Missing\n# ============================================================================\n\nimport os\nfrom google.colab import files\n\n# Required files\nrequired_files = [\n    'cue_families.py',\n    'word_level_analysis.py',\n    'run_comprehensive_audit.py',\n    'analyze_comprehensive_results.py',\n    'stimuli_comprehensive.json',\n    'run_context_ablation.py',\n    'analyze_context_ablation.py'\n]\n\nprint(\"=\" * 80)\nprint(\"CHECKING FOR REQUIRED FILES IN GOOGLE DRIVE\")\nprint(\"=\" * 80)\nprint()\n\nmissing_files = []\nfor f in required_files:\n    if os.path.exists(f):\n        size_kb = os.path.getsize(f) / 1024\n        print(f\"✓ {f:40s} ({size_kb:>6.1f} KB)\")\n    else:\n        print(f\"✗ {f:40s} MISSING\")\n        missing_files.append(f)\n\nprint()\n\nif missing_files:\n    print(f\"⚠️  Missing {len(missing_files)} files\")\n    print()\n    print(\"Please upload the following files:\")\n    for f in missing_files:\n        print(f\"  - {f}\")\n    print()\n    print(\"Click the button below to upload files...\")\n    print(\"(Files will be saved to Google Drive and persist across sessions)\")\n    print()\n\n    uploaded = files.upload()\n\n    print()\n    print(f\"✓ Uploaded {len(uploaded)} files to Google Drive\")\n    print(\"✓ These files will persist - you won't need to upload again!\")\nelse:\n    print(\"✓ All required files present in Google Drive\")\n    print(\"✓ Ready to run experiments!\")\n\nprint()",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that files were uploaded\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'cue_families.py',\n",
    "    'word_level_analysis.py',\n",
    "    'run_comprehensive_audit.py',\n",
    "    'analyze_comprehensive_results.py',\n",
    "    'stimuli_comprehensive.json'\n",
    "]\n",
    "\n",
    "print(\"Checking uploaded files...\\n\")\n",
    "all_present = True\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"✓ {f}\")\n",
    "    else:\n",
    "        print(f\"✗ {f} - MISSING!\")\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n✓ All required files present!\")\n",
    "else:\n",
    "    print(\"\\n✗ Please upload missing files before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Verify GPU Access"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch spacy pandas matplotlib seaborn scipy tqdm\n",
    "\n",
    "# Download spaCy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"\\n✓ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Run Comprehensive Audit (GPT-2)\n\n**Runtime**: ~5-10 minutes with GPU\n\n**Results save to Google Drive automatically!**\n\n**Change model**: Replace `'gpt2'` with:\n- `'gpt2-medium'` (355M)\n- `'gpt2-large'` (774M) - may need Colab Pro\n- `'EleutherAI/pythia-410m'`\n- `'EleutherAI/pythia-160m'`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU found!\")\n",
    "    print(\"  Go to: Runtime → Change runtime type → GPU → Save\")\n",
    "    print(\"  Then re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Analyze Results\n\n**All plots and CSV files save to Google Drive automatically!**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive audit\n",
    "!python run_comprehensive_audit.py \\\n",
    "    --model gpt2 \\\n",
    "    --stimuli stimuli_comprehensive.json \\\n",
    "    --output comprehensive_audit_gpt2.json \\\n",
    "    --method lexicon \\\n",
    "    --top-k 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Display Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run statistical analysis\n",
    "!python analyze_comprehensive_results.py comprehensive_audit_gpt2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View statistical comparisons\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('comprehensive_audit_gpt2_comparisons.csv')\n",
    "print(\"Statistical Comparisons (with FDR correction):\\n\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 8: Download Results to Local Machine (Optional)\n\n**Note**: Results are already safe in Google Drive! This step is optional if you want copies on your local computer.\n\n**To download from Google Drive**:\n1. Open Google Drive in your browser\n2. Go to `My Drive/morphosyntax_experiment/`\n3. Select files and click Download\n\n**Or download directly from Colab** (right-click files in left sidebar → Download):\n- `comprehensive_audit_gpt2.json` - Full results\n- `comprehensive_audit_gpt2_comparisons.csv` - Statistical tests\n- `comprehensive_audit_gpt2_summary.png` - Summary plot\n- `comprehensive_audit_gpt2_infinitival_to_paired.png` - Infinitival TO plot\n- And 5 more family-specific plots..."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display infinitival_to paired plot\n",
    "print(\"Infinitival TO - Paired Comparison:\\n\")\n",
    "display(Image('comprehensive_audit_gpt2_infinitival_to_paired.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "**Right-click these files in the file browser (left sidebar) and select Download:**\n",
    "\n",
    "- `comprehensive_audit_gpt2.json` - Full results\n",
    "- `comprehensive_audit_gpt2_comparisons.csv` - Statistical tests\n",
    "- `comprehensive_audit_gpt2_summary.png` - Summary plot\n",
    "- `comprehensive_audit_gpt2_infinitival_to_paired.png` - Infinitival TO plot\n",
    "- And 5 more family-specific plots..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Context Ablation Analysis\n",
    "\n",
    "**Tests**: How much of the effect is cue-driven vs. scaffold-dependent?\n",
    "\n",
    "**Runtime**: ~3-5 minutes with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run context ablation (if files uploaded)\n",
    "!python run_context_ablation.py \\\n",
    "    --model gpt2 \\\n",
    "    --stimuli stimuli_comprehensive.json \\\n",
    "    --output context_ablation_gpt2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ablation results\n",
    "!python analyze_context_ablation.py context_ablation_gpt2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ablation results\n",
    "print(\"Infinitival TO - Context Ablation:\\n\")\n",
    "display(Image('context_ablation_gpt2_infinitival_to_ablation_plot.png'))\n",
    "\n",
    "print(\"\\nDeterminers - Context Ablation:\\n\")\n",
    "display(Image('context_ablation_gpt2_determiners_ablation_plot.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\n**Out of memory**:\n- Use smaller model (gpt2 instead of gpt2-large)\n- Or upgrade to Colab Pro (more GPU memory)\n\n**Runtime disconnected**:\n- ✅ **No worries!** All results are saved to Google Drive\n- Just reconnect, run Step 1 (mount Drive), and continue\n- Previous results are preserved in `My Drive/morphosyntax_experiment/`\n- Colab Pro has longer limits (24h vs 12h) if you want to run large models\n\n**Slow execution**:\n- Make sure GPU is enabled (see Step 4)\n- Reduce --top-k to 100 (faster but less precise)\n\n**Files not found after reconnecting**:\n- Run Step 1 again to re-mount Google Drive\n- Check `My Drive/morphosyntax_experiment/` in Google Drive web interface\n- Files persist across sessions - no need to re-upload!\n\n**Authorization popup**:\n- This is normal the first time you mount Google Drive\n- Click the link → Sign in → Copy code → Paste back\n- You only need to do this once per Google account"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models (takes ~30-50 minutes total with GPU)\n",
    "models = ['gpt2', 'gpt2-medium', 'EleutherAI/pythia-410m']\n",
    "\n",
    "for model in models:\n",
    "    model_slug = model.replace('/', '_')\n",
    "    output_file = f'comprehensive_audit_{model_slug}.json'\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running: {model}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Run audit\n",
    "    !python run_comprehensive_audit.py \\\n",
    "        --model {model} \\\n",
    "        --stimuli stimuli_comprehensive.json \\\n",
    "        --output {output_file}\n",
    "    \n",
    "    # Analyze\n",
    "    !python analyze_comprehensive_results.py {output_file}\n",
    "    \n",
    "    print(f\"\\n✓ {model} complete!\\n\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL MODELS COMPLETE!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Out of memory**:\n",
    "- Use smaller model (gpt2 instead of gpt2-large)\n",
    "- Or upgrade to Colab Pro (more GPU memory)\n",
    "\n",
    "**Runtime disconnected**:\n",
    "- Colab Pro has longer limits (24h vs 12h)\n",
    "- Results are saved progressively - check file browser\n",
    "\n",
    "**Slow execution**:\n",
    "- Make sure GPU is enabled (see Step 3)\n",
    "- Reduce --top-k to 100 (faster but less precise)\n",
    "\n",
    "**Files not found**:\n",
    "- Re-upload files (Colab clears files when disconnected)\n",
    "- Save results to Google Drive to persist across sessions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}